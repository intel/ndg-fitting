//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-27506705
// Cuda compilation tools, release 10.2, V10.2.89
// Based on LLVM 3.4svn
//

.version 6.5
.target sm_70
.address_size 64

	// .globl	reduce_sum_i32
.extern .shared .align 4 .b8 shared[];
.extern .shared .align 8 .b8 shared_d[];

.visible .entry reduce_sum_i32(
	.param .u64 reduce_sum_i32_param_0,
	.param .u32 reduce_sum_i32_param_1,
	.param .u64 reduce_sum_i32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_sum_i32_param_0];
	ld.param.u32 	%r21, [reduce_sum_i32_param_1];
	ld.param.u64 	%rd2, [reduce_sum_i32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, 0;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB0_4;

BB0_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	add.s32 	%r62, %r27, %r62;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB0_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	add.s32 	%r62, %r28, %r62;

BB0_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB0_1;

BB0_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB0_6;

	ld.shared.u32 	%r33, [%r11+2048];
	add.s32 	%r62, %r33, %r62;
	st.shared.u32 	[%r11], %r62;

BB0_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB0_8;

	ld.shared.u32 	%r35, [%r11+1024];
	add.s32 	%r62, %r35, %r62;
	st.shared.u32 	[%r11], %r62;

BB0_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB0_10;

	ld.shared.u32 	%r37, [%r11+512];
	add.s32 	%r62, %r37, %r62;
	st.shared.u32 	[%r11], %r62;

BB0_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB0_12;

	ld.shared.u32 	%r39, [%r11+256];
	add.s32 	%r62, %r39, %r62;
	st.shared.u32 	[%r11], %r62;

BB0_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB0_15;

	ld.shared.u32 	%r41, [%r11+128];
	add.s32 	%r42, %r41, %r62;
	mov.u32 	%r43, 2;
	mov.u32 	%r44, 31;
	mov.u32 	%r45, 16;
	mov.u32 	%r46, -1;
	shfl.sync.down.b32 	%r47|%p9, %r42, %r45, %r44, %r46;
	add.s32 	%r48, %r47, %r42;
	mov.u32 	%r49, 8;
	shfl.sync.down.b32 	%r50|%p10, %r48, %r49, %r44, %r46;
	add.s32 	%r51, %r50, %r48;
	mov.u32 	%r52, 4;
	shfl.sync.down.b32 	%r53|%p11, %r51, %r52, %r44, %r46;
	add.s32 	%r54, %r53, %r51;
	shfl.sync.down.b32 	%r55|%p12, %r54, %r43, %r44, %r46;
	add.s32 	%r56, %r55, %r54;
	mov.u32 	%r57, 1;
	shfl.sync.down.b32 	%r58|%p13, %r56, %r57, %r44, %r46;
	add.s32 	%r20, %r58, %r56;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB0_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB0_15:
	ret;
}

	// .globl	reduce_sum_u32
.visible .entry reduce_sum_u32(
	.param .u64 reduce_sum_u32_param_0,
	.param .u32 reduce_sum_u32_param_1,
	.param .u64 reduce_sum_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_sum_u32_param_0];
	ld.param.u32 	%r21, [reduce_sum_u32_param_1];
	ld.param.u64 	%rd2, [reduce_sum_u32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, 0;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB1_4;

BB1_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	add.s32 	%r62, %r27, %r62;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB1_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	add.s32 	%r62, %r28, %r62;

BB1_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB1_1;

BB1_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB1_6;

	ld.shared.u32 	%r33, [%r11+2048];
	add.s32 	%r62, %r33, %r62;
	st.shared.u32 	[%r11], %r62;

BB1_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB1_8;

	ld.shared.u32 	%r35, [%r11+1024];
	add.s32 	%r62, %r35, %r62;
	st.shared.u32 	[%r11], %r62;

BB1_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB1_10;

	ld.shared.u32 	%r37, [%r11+512];
	add.s32 	%r62, %r37, %r62;
	st.shared.u32 	[%r11], %r62;

BB1_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB1_12;

	ld.shared.u32 	%r39, [%r11+256];
	add.s32 	%r62, %r39, %r62;
	st.shared.u32 	[%r11], %r62;

BB1_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB1_15;

	ld.shared.u32 	%r41, [%r11+128];
	add.s32 	%r42, %r41, %r62;
	mov.u32 	%r43, 2;
	mov.u32 	%r44, 31;
	mov.u32 	%r45, 16;
	mov.u32 	%r46, -1;
	shfl.sync.down.b32 	%r47|%p9, %r42, %r45, %r44, %r46;
	add.s32 	%r48, %r47, %r42;
	mov.u32 	%r49, 8;
	shfl.sync.down.b32 	%r50|%p10, %r48, %r49, %r44, %r46;
	add.s32 	%r51, %r50, %r48;
	mov.u32 	%r52, 4;
	shfl.sync.down.b32 	%r53|%p11, %r51, %r52, %r44, %r46;
	add.s32 	%r54, %r53, %r51;
	shfl.sync.down.b32 	%r55|%p12, %r54, %r43, %r44, %r46;
	add.s32 	%r56, %r55, %r54;
	mov.u32 	%r57, 1;
	shfl.sync.down.b32 	%r58|%p13, %r56, %r57, %r44, %r46;
	add.s32 	%r20, %r58, %r56;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB1_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB1_15:
	ret;
}

	// .globl	reduce_sum_i64
.visible .entry reduce_sum_i64(
	.param .u64 reduce_sum_i64_param_0,
	.param .u32 reduce_sum_i64_param_1,
	.param .u64 reduce_sum_i64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd19, [reduce_sum_i64_param_0];
	ld.param.u32 	%r9, [reduce_sum_i64_param_1];
	ld.param.u64 	%rd16, [reduce_sum_i64_param_2];
	cvta.to.global.u64 	%rd1, %rd19;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.u64 	%rd44, 0;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB2_4;

BB2_1:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	add.s64 	%rd44, %rd22, %rd44;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB2_3;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	add.s64 	%rd44, %rd25, %rd44;

BB2_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB2_1;

BB2_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB2_6;

	ld.shared.u64 	%rd26, [%r8+4096];
	add.s64 	%rd44, %rd26, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB2_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB2_8;

	ld.shared.u64 	%rd27, [%r8+2048];
	add.s64 	%rd44, %rd27, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB2_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB2_10;

	ld.shared.u64 	%rd28, [%r8+1024];
	add.s64 	%rd44, %rd28, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB2_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB2_12;

	ld.shared.u64 	%rd29, [%r8+512];
	add.s64 	%rd44, %rd29, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB2_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB2_15;

	ld.shared.u64 	%rd40, [%r8+256];
	add.s64 	%rd30, %rd40, %rd44;
	// inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// inline asm
	mov.b64 %rd31, {%r16,%r17};
	// inline asm
	add.s64 	%rd32, %rd31, %rd30;
	// inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// inline asm
	mov.b64 %rd33, {%r20,%r21};
	// inline asm
	add.s64 	%rd34, %rd33, %rd32;
	// inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// inline asm
	mov.b64 %rd35, {%r24,%r25};
	// inline asm
	add.s64 	%rd36, %rd35, %rd34;
	// inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// inline asm
	mov.b64 %rd37, {%r28,%r29};
	// inline asm
	add.s64 	%rd38, %rd37, %rd36;
	// inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// inline asm
	mov.b64 %rd39, {%r32,%r33};
	// inline asm
	add.s64 	%rd15, %rd39, %rd38;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB2_15;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

BB2_15:
	ret;
}

	// .globl	reduce_sum_u64
.visible .entry reduce_sum_u64(
	.param .u64 reduce_sum_u64_param_0,
	.param .u32 reduce_sum_u64_param_1,
	.param .u64 reduce_sum_u64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd19, [reduce_sum_u64_param_0];
	ld.param.u32 	%r9, [reduce_sum_u64_param_1];
	ld.param.u64 	%rd16, [reduce_sum_u64_param_2];
	cvta.to.global.u64 	%rd1, %rd19;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.u64 	%rd44, 0;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB3_4;

BB3_1:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	add.s64 	%rd44, %rd22, %rd44;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB3_3;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	add.s64 	%rd44, %rd25, %rd44;

BB3_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB3_1;

BB3_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB3_6;

	ld.shared.u64 	%rd26, [%r8+4096];
	add.s64 	%rd44, %rd26, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB3_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB3_8;

	ld.shared.u64 	%rd27, [%r8+2048];
	add.s64 	%rd44, %rd27, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB3_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB3_10;

	ld.shared.u64 	%rd28, [%r8+1024];
	add.s64 	%rd44, %rd28, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB3_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB3_12;

	ld.shared.u64 	%rd29, [%r8+512];
	add.s64 	%rd44, %rd29, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB3_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB3_15;

	ld.shared.u64 	%rd40, [%r8+256];
	add.s64 	%rd30, %rd40, %rd44;
	// inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// inline asm
	mov.b64 %rd31, {%r16,%r17};
	// inline asm
	add.s64 	%rd32, %rd31, %rd30;
	// inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// inline asm
	mov.b64 %rd33, {%r20,%r21};
	// inline asm
	add.s64 	%rd34, %rd33, %rd32;
	// inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// inline asm
	mov.b64 %rd35, {%r24,%r25};
	// inline asm
	add.s64 	%rd36, %rd35, %rd34;
	// inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// inline asm
	mov.b64 %rd37, {%r28,%r29};
	// inline asm
	add.s64 	%rd38, %rd37, %rd36;
	// inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// inline asm
	mov.b64 %rd39, {%r32,%r33};
	// inline asm
	add.s64 	%rd15, %rd39, %rd38;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB3_15;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

BB3_15:
	ret;
}

	// .globl	reduce_sum_f32
.visible .entry reduce_sum_f32(
	.param .u64 reduce_sum_f32_param_0,
	.param .u32 reduce_sum_f32_param_1,
	.param .u64 reduce_sum_f32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<41>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_sum_f32_param_0];
	ld.param.u32 	%r7, [reduce_sum_f32_param_1];
	ld.param.u64 	%rd2, [reduce_sum_f32_param_2];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	add.s32 	%r38, %r10, %r8;
	mov.f32 	%f34, 0f00000000;
	setp.ge.u32	%p1, %r38, %r7;
	@%p1 bra 	BB4_4;

BB4_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r38, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f17, [%rd5];
	add.f32 	%f34, %f34, %f17;
	add.s32 	%r3, %r38, 1024;
	setp.ge.u32	%p2, %r3, %r7;
	@%p2 bra 	BB4_3;

	mul.wide.u32 	%rd7, %r3, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.f32 	%f18, [%rd8];
	add.f32 	%f34, %f34, %f18;

BB4_3:
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r12, %r11, 11;
	add.s32 	%r38, %r38, %r12;
	setp.lt.u32	%p3, %r38, %r7;
	@%p3 bra 	BB4_1;

BB4_4:
	shl.b32 	%r13, %r8, 2;
	mov.u32 	%r14, shared;
	add.s32 	%r6, %r14, %r13;
	st.shared.f32 	[%r6], %f34;
	bar.sync 	0;
	setp.gt.u32	%p4, %r8, 511;
	@%p4 bra 	BB4_6;

	ld.shared.f32 	%f19, [%r6+2048];
	add.f32 	%f34, %f34, %f19;
	st.shared.f32 	[%r6], %f34;

BB4_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r8, 255;
	@%p5 bra 	BB4_8;

	ld.shared.f32 	%f20, [%r6+1024];
	add.f32 	%f34, %f34, %f20;
	st.shared.f32 	[%r6], %f34;

BB4_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r8, 127;
	@%p6 bra 	BB4_10;

	ld.shared.f32 	%f21, [%r6+512];
	add.f32 	%f34, %f34, %f21;
	st.shared.f32 	[%r6], %f34;

BB4_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r8, 63;
	@%p7 bra 	BB4_12;

	ld.shared.f32 	%f22, [%r6+256];
	add.f32 	%f34, %f34, %f22;
	st.shared.f32 	[%r6], %f34;

BB4_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r8, 31;
	@%p8 bra 	BB4_15;

	ld.shared.f32 	%f23, [%r6+128];
	add.f32 	%f24, %f34, %f23;
	mov.b32 	 %r19, %f24;
	mov.u32 	%r20, 2;
	mov.u32 	%r21, 31;
	mov.u32 	%r22, 16;
	mov.u32 	%r23, -1;
	shfl.sync.down.b32 	%r24|%p9, %r19, %r22, %r21, %r23;
	mov.b32 	 %f25, %r24;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	 %r25, %f26;
	mov.u32 	%r26, 8;
	shfl.sync.down.b32 	%r27|%p10, %r25, %r26, %r21, %r23;
	mov.b32 	 %f27, %r27;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	 %r28, %f28;
	mov.u32 	%r29, 4;
	shfl.sync.down.b32 	%r30|%p11, %r28, %r29, %r21, %r23;
	mov.b32 	 %f29, %r30;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	 %r31, %f30;
	shfl.sync.down.b32 	%r32|%p12, %r31, %r20, %r21, %r23;
	mov.b32 	 %f31, %r32;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	 %r33, %f32;
	mov.u32 	%r34, 1;
	shfl.sync.down.b32 	%r35|%p13, %r33, %r34, %r21, %r23;
	mov.b32 	 %f33, %r35;
	add.f32 	%f14, %f32, %f33;
	setp.ne.s32	%p14, %r8, 0;
	@%p14 bra 	BB4_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r9, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.f32 	[%rd11], %f14;

BB4_15:
	ret;
}

	// .globl	reduce_sum_f64
.visible .entry reduce_sum_f64(
	.param .u64 reduce_sum_f64_param_0,
	.param .u32 reduce_sum_f64_param_1,
	.param .u64 reduce_sum_f64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<41>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_sum_f64_param_0];
	ld.param.u32 	%r9, [reduce_sum_f64_param_1];
	ld.param.u64 	%rd2, [reduce_sum_f64_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.f64 	%fd34, 0d0000000000000000;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB5_4;

BB5_1:
	mul.wide.u32 	%rd4, %r41, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f64 	%fd17, [%rd5];
	add.f64 	%fd34, %fd34, %fd17;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB5_3;

	mul.wide.u32 	%rd6, %r6, 8;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f64 	%fd18, [%rd7];
	add.f64 	%fd34, %fd34, %fd18;

BB5_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB5_1;

BB5_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared_d;
	add.s32 	%r8, %r13, %r12;
	st.shared.f64 	[%r8], %fd34;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB5_6;

	ld.shared.f64 	%fd19, [%r8+4096];
	add.f64 	%fd34, %fd34, %fd19;
	st.shared.f64 	[%r8], %fd34;

BB5_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB5_8;

	ld.shared.f64 	%fd20, [%r8+2048];
	add.f64 	%fd34, %fd34, %fd20;
	st.shared.f64 	[%r8], %fd34;

BB5_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB5_10;

	ld.shared.f64 	%fd21, [%r8+1024];
	add.f64 	%fd34, %fd34, %fd21;
	st.shared.f64 	[%r8], %fd34;

BB5_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB5_12;

	ld.shared.f64 	%fd22, [%r8+512];
	add.f64 	%fd34, %fd34, %fd22;
	st.shared.f64 	[%r8], %fd34;

BB5_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB5_15;

	ld.shared.f64 	%fd33, [%r8+256];
	add.f64 	%fd23, %fd34, %fd33;
	// inline asm
	mov.b64 {%r14,%r15}, %fd23;
	// inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// inline asm
	mov.b64 %fd24, {%r16,%r17};
	// inline asm
	add.f64 	%fd25, %fd23, %fd24;
	// inline asm
	mov.b64 {%r18,%r19}, %fd25;
	// inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// inline asm
	mov.b64 %fd26, {%r20,%r21};
	// inline asm
	add.f64 	%fd27, %fd25, %fd26;
	// inline asm
	mov.b64 {%r22,%r23}, %fd27;
	// inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// inline asm
	mov.b64 %fd28, {%r24,%r25};
	// inline asm
	add.f64 	%fd29, %fd27, %fd28;
	// inline asm
	mov.b64 {%r26,%r27}, %fd29;
	// inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// inline asm
	mov.b64 %fd30, {%r28,%r29};
	// inline asm
	add.f64 	%fd31, %fd29, %fd30;
	// inline asm
	mov.b64 {%r30,%r31}, %fd31;
	// inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// inline asm
	mov.b64 %fd32, {%r32,%r33};
	// inline asm
	add.f64 	%fd14, %fd31, %fd32;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB5_15;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r1, 8;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f64 	[%rd10], %fd14;

BB5_15:
	ret;
}

	// .globl	reduce_mul_i32
.visible .entry reduce_mul_i32(
	.param .u64 reduce_mul_i32_param_0,
	.param .u32 reduce_mul_i32_param_1,
	.param .u64 reduce_mul_i32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_mul_i32_param_0];
	ld.param.u32 	%r21, [reduce_mul_i32_param_1];
	ld.param.u64 	%rd2, [reduce_mul_i32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, 1;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB6_4;

BB6_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	mul.lo.s32 	%r62, %r27, %r62;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB6_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	mul.lo.s32 	%r62, %r28, %r62;

BB6_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB6_1;

BB6_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB6_6;

	ld.shared.u32 	%r33, [%r11+2048];
	mul.lo.s32 	%r62, %r33, %r62;
	st.shared.u32 	[%r11], %r62;

BB6_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB6_8;

	ld.shared.u32 	%r35, [%r11+1024];
	mul.lo.s32 	%r62, %r35, %r62;
	st.shared.u32 	[%r11], %r62;

BB6_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB6_10;

	ld.shared.u32 	%r37, [%r11+512];
	mul.lo.s32 	%r62, %r37, %r62;
	st.shared.u32 	[%r11], %r62;

BB6_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB6_12;

	ld.shared.u32 	%r39, [%r11+256];
	mul.lo.s32 	%r62, %r39, %r62;
	st.shared.u32 	[%r11], %r62;

BB6_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB6_15;

	ld.shared.u32 	%r41, [%r11+128];
	mul.lo.s32 	%r42, %r41, %r62;
	mov.u32 	%r43, 2;
	mov.u32 	%r44, 31;
	mov.u32 	%r45, 16;
	mov.u32 	%r46, -1;
	shfl.sync.down.b32 	%r47|%p9, %r42, %r45, %r44, %r46;
	mul.lo.s32 	%r48, %r47, %r42;
	mov.u32 	%r49, 8;
	shfl.sync.down.b32 	%r50|%p10, %r48, %r49, %r44, %r46;
	mul.lo.s32 	%r51, %r50, %r48;
	mov.u32 	%r52, 4;
	shfl.sync.down.b32 	%r53|%p11, %r51, %r52, %r44, %r46;
	mul.lo.s32 	%r54, %r53, %r51;
	shfl.sync.down.b32 	%r55|%p12, %r54, %r43, %r44, %r46;
	mul.lo.s32 	%r56, %r55, %r54;
	mov.u32 	%r57, 1;
	shfl.sync.down.b32 	%r58|%p13, %r56, %r57, %r44, %r46;
	mul.lo.s32 	%r20, %r58, %r56;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB6_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB6_15:
	ret;
}

	// .globl	reduce_mul_u32
.visible .entry reduce_mul_u32(
	.param .u64 reduce_mul_u32_param_0,
	.param .u32 reduce_mul_u32_param_1,
	.param .u64 reduce_mul_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_mul_u32_param_0];
	ld.param.u32 	%r21, [reduce_mul_u32_param_1];
	ld.param.u64 	%rd2, [reduce_mul_u32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, 1;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB7_4;

BB7_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	mul.lo.s32 	%r62, %r27, %r62;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB7_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	mul.lo.s32 	%r62, %r28, %r62;

BB7_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB7_1;

BB7_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB7_6;

	ld.shared.u32 	%r33, [%r11+2048];
	mul.lo.s32 	%r62, %r33, %r62;
	st.shared.u32 	[%r11], %r62;

BB7_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB7_8;

	ld.shared.u32 	%r35, [%r11+1024];
	mul.lo.s32 	%r62, %r35, %r62;
	st.shared.u32 	[%r11], %r62;

BB7_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB7_10;

	ld.shared.u32 	%r37, [%r11+512];
	mul.lo.s32 	%r62, %r37, %r62;
	st.shared.u32 	[%r11], %r62;

BB7_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB7_12;

	ld.shared.u32 	%r39, [%r11+256];
	mul.lo.s32 	%r62, %r39, %r62;
	st.shared.u32 	[%r11], %r62;

BB7_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB7_15;

	ld.shared.u32 	%r41, [%r11+128];
	mul.lo.s32 	%r42, %r41, %r62;
	mov.u32 	%r43, 2;
	mov.u32 	%r44, 31;
	mov.u32 	%r45, 16;
	mov.u32 	%r46, -1;
	shfl.sync.down.b32 	%r47|%p9, %r42, %r45, %r44, %r46;
	mul.lo.s32 	%r48, %r47, %r42;
	mov.u32 	%r49, 8;
	shfl.sync.down.b32 	%r50|%p10, %r48, %r49, %r44, %r46;
	mul.lo.s32 	%r51, %r50, %r48;
	mov.u32 	%r52, 4;
	shfl.sync.down.b32 	%r53|%p11, %r51, %r52, %r44, %r46;
	mul.lo.s32 	%r54, %r53, %r51;
	shfl.sync.down.b32 	%r55|%p12, %r54, %r43, %r44, %r46;
	mul.lo.s32 	%r56, %r55, %r54;
	mov.u32 	%r57, 1;
	shfl.sync.down.b32 	%r58|%p13, %r56, %r57, %r44, %r46;
	mul.lo.s32 	%r20, %r58, %r56;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB7_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB7_15:
	ret;
}

	// .globl	reduce_mul_i64
.visible .entry reduce_mul_i64(
	.param .u64 reduce_mul_i64_param_0,
	.param .u32 reduce_mul_i64_param_1,
	.param .u64 reduce_mul_i64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd19, [reduce_mul_i64_param_0];
	ld.param.u32 	%r9, [reduce_mul_i64_param_1];
	ld.param.u64 	%rd16, [reduce_mul_i64_param_2];
	cvta.to.global.u64 	%rd1, %rd19;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.u64 	%rd44, 1;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB8_4;

BB8_1:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	mul.lo.s64 	%rd44, %rd22, %rd44;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB8_3;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	mul.lo.s64 	%rd44, %rd25, %rd44;

BB8_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB8_1;

BB8_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB8_6;

	ld.shared.u64 	%rd26, [%r8+4096];
	mul.lo.s64 	%rd44, %rd26, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB8_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB8_8;

	ld.shared.u64 	%rd27, [%r8+2048];
	mul.lo.s64 	%rd44, %rd27, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB8_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB8_10;

	ld.shared.u64 	%rd28, [%r8+1024];
	mul.lo.s64 	%rd44, %rd28, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB8_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB8_12;

	ld.shared.u64 	%rd29, [%r8+512];
	mul.lo.s64 	%rd44, %rd29, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB8_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB8_15;

	ld.shared.u64 	%rd40, [%r8+256];
	mul.lo.s64 	%rd30, %rd40, %rd44;
	// inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// inline asm
	mov.b64 %rd31, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd32, %rd31, %rd30;
	// inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// inline asm
	mov.b64 %rd33, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd34, %rd33, %rd32;
	// inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// inline asm
	mov.b64 %rd35, {%r24,%r25};
	// inline asm
	mul.lo.s64 	%rd36, %rd35, %rd34;
	// inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// inline asm
	mov.b64 %rd37, {%r28,%r29};
	// inline asm
	mul.lo.s64 	%rd38, %rd37, %rd36;
	// inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// inline asm
	mov.b64 %rd39, {%r32,%r33};
	// inline asm
	mul.lo.s64 	%rd15, %rd39, %rd38;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB8_15;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

BB8_15:
	ret;
}

	// .globl	reduce_mul_u64
.visible .entry reduce_mul_u64(
	.param .u64 reduce_mul_u64_param_0,
	.param .u32 reduce_mul_u64_param_1,
	.param .u64 reduce_mul_u64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd19, [reduce_mul_u64_param_0];
	ld.param.u32 	%r9, [reduce_mul_u64_param_1];
	ld.param.u64 	%rd16, [reduce_mul_u64_param_2];
	cvta.to.global.u64 	%rd1, %rd19;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.u64 	%rd44, 1;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB9_4;

BB9_1:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	mul.lo.s64 	%rd44, %rd22, %rd44;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB9_3;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	mul.lo.s64 	%rd44, %rd25, %rd44;

BB9_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB9_1;

BB9_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB9_6;

	ld.shared.u64 	%rd26, [%r8+4096];
	mul.lo.s64 	%rd44, %rd26, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB9_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB9_8;

	ld.shared.u64 	%rd27, [%r8+2048];
	mul.lo.s64 	%rd44, %rd27, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB9_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB9_10;

	ld.shared.u64 	%rd28, [%r8+1024];
	mul.lo.s64 	%rd44, %rd28, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB9_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB9_12;

	ld.shared.u64 	%rd29, [%r8+512];
	mul.lo.s64 	%rd44, %rd29, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB9_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB9_15;

	ld.shared.u64 	%rd40, [%r8+256];
	mul.lo.s64 	%rd30, %rd40, %rd44;
	// inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// inline asm
	mov.b64 %rd31, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd32, %rd31, %rd30;
	// inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// inline asm
	mov.b64 %rd33, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd34, %rd33, %rd32;
	// inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// inline asm
	mov.b64 %rd35, {%r24,%r25};
	// inline asm
	mul.lo.s64 	%rd36, %rd35, %rd34;
	// inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// inline asm
	mov.b64 %rd37, {%r28,%r29};
	// inline asm
	mul.lo.s64 	%rd38, %rd37, %rd36;
	// inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// inline asm
	mov.b64 %rd39, {%r32,%r33};
	// inline asm
	mul.lo.s64 	%rd15, %rd39, %rd38;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB9_15;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

BB9_15:
	ret;
}

	// .globl	reduce_mul_f32
.visible .entry reduce_mul_f32(
	.param .u64 reduce_mul_f32_param_0,
	.param .u32 reduce_mul_f32_param_1,
	.param .u64 reduce_mul_f32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<41>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_mul_f32_param_0];
	ld.param.u32 	%r7, [reduce_mul_f32_param_1];
	ld.param.u64 	%rd2, [reduce_mul_f32_param_2];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	add.s32 	%r38, %r10, %r8;
	mov.f32 	%f34, 0f3F800000;
	setp.ge.u32	%p1, %r38, %r7;
	@%p1 bra 	BB10_4;

BB10_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r38, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f17, [%rd5];
	mul.f32 	%f34, %f34, %f17;
	add.s32 	%r3, %r38, 1024;
	setp.ge.u32	%p2, %r3, %r7;
	@%p2 bra 	BB10_3;

	mul.wide.u32 	%rd7, %r3, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.f32 	%f18, [%rd8];
	mul.f32 	%f34, %f34, %f18;

BB10_3:
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r12, %r11, 11;
	add.s32 	%r38, %r38, %r12;
	setp.lt.u32	%p3, %r38, %r7;
	@%p3 bra 	BB10_1;

BB10_4:
	shl.b32 	%r13, %r8, 2;
	mov.u32 	%r14, shared;
	add.s32 	%r6, %r14, %r13;
	st.shared.f32 	[%r6], %f34;
	bar.sync 	0;
	setp.gt.u32	%p4, %r8, 511;
	@%p4 bra 	BB10_6;

	ld.shared.f32 	%f19, [%r6+2048];
	mul.f32 	%f34, %f34, %f19;
	st.shared.f32 	[%r6], %f34;

BB10_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r8, 255;
	@%p5 bra 	BB10_8;

	ld.shared.f32 	%f20, [%r6+1024];
	mul.f32 	%f34, %f34, %f20;
	st.shared.f32 	[%r6], %f34;

BB10_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r8, 127;
	@%p6 bra 	BB10_10;

	ld.shared.f32 	%f21, [%r6+512];
	mul.f32 	%f34, %f34, %f21;
	st.shared.f32 	[%r6], %f34;

BB10_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r8, 63;
	@%p7 bra 	BB10_12;

	ld.shared.f32 	%f22, [%r6+256];
	mul.f32 	%f34, %f34, %f22;
	st.shared.f32 	[%r6], %f34;

BB10_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r8, 31;
	@%p8 bra 	BB10_15;

	ld.shared.f32 	%f23, [%r6+128];
	mul.f32 	%f24, %f34, %f23;
	mov.b32 	 %r19, %f24;
	mov.u32 	%r20, 2;
	mov.u32 	%r21, 31;
	mov.u32 	%r22, 16;
	mov.u32 	%r23, -1;
	shfl.sync.down.b32 	%r24|%p9, %r19, %r22, %r21, %r23;
	mov.b32 	 %f25, %r24;
	mul.f32 	%f26, %f24, %f25;
	mov.b32 	 %r25, %f26;
	mov.u32 	%r26, 8;
	shfl.sync.down.b32 	%r27|%p10, %r25, %r26, %r21, %r23;
	mov.b32 	 %f27, %r27;
	mul.f32 	%f28, %f26, %f27;
	mov.b32 	 %r28, %f28;
	mov.u32 	%r29, 4;
	shfl.sync.down.b32 	%r30|%p11, %r28, %r29, %r21, %r23;
	mov.b32 	 %f29, %r30;
	mul.f32 	%f30, %f28, %f29;
	mov.b32 	 %r31, %f30;
	shfl.sync.down.b32 	%r32|%p12, %r31, %r20, %r21, %r23;
	mov.b32 	 %f31, %r32;
	mul.f32 	%f32, %f30, %f31;
	mov.b32 	 %r33, %f32;
	mov.u32 	%r34, 1;
	shfl.sync.down.b32 	%r35|%p13, %r33, %r34, %r21, %r23;
	mov.b32 	 %f33, %r35;
	mul.f32 	%f14, %f32, %f33;
	setp.ne.s32	%p14, %r8, 0;
	@%p14 bra 	BB10_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r9, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.f32 	[%rd11], %f14;

BB10_15:
	ret;
}

	// .globl	reduce_mul_f64
.visible .entry reduce_mul_f64(
	.param .u64 reduce_mul_f64_param_0,
	.param .u32 reduce_mul_f64_param_1,
	.param .u64 reduce_mul_f64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<41>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_mul_f64_param_0];
	ld.param.u32 	%r9, [reduce_mul_f64_param_1];
	ld.param.u64 	%rd2, [reduce_mul_f64_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.f64 	%fd34, 0d3FF0000000000000;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB11_4;

BB11_1:
	mul.wide.u32 	%rd4, %r41, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f64 	%fd17, [%rd5];
	mul.f64 	%fd34, %fd34, %fd17;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB11_3;

	mul.wide.u32 	%rd6, %r6, 8;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f64 	%fd18, [%rd7];
	mul.f64 	%fd34, %fd34, %fd18;

BB11_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB11_1;

BB11_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared_d;
	add.s32 	%r8, %r13, %r12;
	st.shared.f64 	[%r8], %fd34;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB11_6;

	ld.shared.f64 	%fd19, [%r8+4096];
	mul.f64 	%fd34, %fd34, %fd19;
	st.shared.f64 	[%r8], %fd34;

BB11_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB11_8;

	ld.shared.f64 	%fd20, [%r8+2048];
	mul.f64 	%fd34, %fd34, %fd20;
	st.shared.f64 	[%r8], %fd34;

BB11_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB11_10;

	ld.shared.f64 	%fd21, [%r8+1024];
	mul.f64 	%fd34, %fd34, %fd21;
	st.shared.f64 	[%r8], %fd34;

BB11_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB11_12;

	ld.shared.f64 	%fd22, [%r8+512];
	mul.f64 	%fd34, %fd34, %fd22;
	st.shared.f64 	[%r8], %fd34;

BB11_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB11_15;

	ld.shared.f64 	%fd33, [%r8+256];
	mul.f64 	%fd23, %fd34, %fd33;
	// inline asm
	mov.b64 {%r14,%r15}, %fd23;
	// inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// inline asm
	mov.b64 %fd24, {%r16,%r17};
	// inline asm
	mul.f64 	%fd25, %fd23, %fd24;
	// inline asm
	mov.b64 {%r18,%r19}, %fd25;
	// inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// inline asm
	mov.b64 %fd26, {%r20,%r21};
	// inline asm
	mul.f64 	%fd27, %fd25, %fd26;
	// inline asm
	mov.b64 {%r22,%r23}, %fd27;
	// inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// inline asm
	mov.b64 %fd28, {%r24,%r25};
	// inline asm
	mul.f64 	%fd29, %fd27, %fd28;
	// inline asm
	mov.b64 {%r26,%r27}, %fd29;
	// inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// inline asm
	mov.b64 %fd30, {%r28,%r29};
	// inline asm
	mul.f64 	%fd31, %fd29, %fd30;
	// inline asm
	mov.b64 {%r30,%r31}, %fd31;
	// inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// inline asm
	mov.b64 %fd32, {%r32,%r33};
	// inline asm
	mul.f64 	%fd14, %fd31, %fd32;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB11_15;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r1, 8;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f64 	[%rd10], %fd14;

BB11_15:
	ret;
}

	// .globl	reduce_min_i32
.visible .entry reduce_min_i32(
	.param .u64 reduce_min_i32_param_0,
	.param .u32 reduce_min_i32_param_1,
	.param .u64 reduce_min_i32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_min_i32_param_0];
	ld.param.u32 	%r21, [reduce_min_i32_param_1];
	ld.param.u64 	%rd2, [reduce_min_i32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, 2147483647;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB12_4;

BB12_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	min.s32 	%r62, %r62, %r27;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB12_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	min.s32 	%r62, %r62, %r28;

BB12_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB12_1;

BB12_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB12_6;

	ld.shared.u32 	%r33, [%r11+2048];
	min.s32 	%r62, %r62, %r33;
	st.shared.u32 	[%r11], %r62;

BB12_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB12_8;

	ld.shared.u32 	%r35, [%r11+1024];
	min.s32 	%r62, %r62, %r35;
	st.shared.u32 	[%r11], %r62;

BB12_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB12_10;

	ld.shared.u32 	%r37, [%r11+512];
	min.s32 	%r62, %r62, %r37;
	st.shared.u32 	[%r11], %r62;

BB12_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB12_12;

	ld.shared.u32 	%r39, [%r11+256];
	min.s32 	%r62, %r62, %r39;
	st.shared.u32 	[%r11], %r62;

BB12_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB12_15;

	ld.shared.u32 	%r41, [%r11+128];
	min.s32 	%r42, %r62, %r41;
	mov.u32 	%r43, 2;
	mov.u32 	%r44, 31;
	mov.u32 	%r45, 16;
	mov.u32 	%r46, -1;
	shfl.sync.down.b32 	%r47|%p9, %r42, %r45, %r44, %r46;
	min.s32 	%r48, %r42, %r47;
	mov.u32 	%r49, 8;
	shfl.sync.down.b32 	%r50|%p10, %r48, %r49, %r44, %r46;
	min.s32 	%r51, %r48, %r50;
	mov.u32 	%r52, 4;
	shfl.sync.down.b32 	%r53|%p11, %r51, %r52, %r44, %r46;
	min.s32 	%r54, %r51, %r53;
	shfl.sync.down.b32 	%r55|%p12, %r54, %r43, %r44, %r46;
	min.s32 	%r56, %r54, %r55;
	mov.u32 	%r57, 1;
	shfl.sync.down.b32 	%r58|%p13, %r56, %r57, %r44, %r46;
	min.s32 	%r20, %r56, %r58;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB12_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB12_15:
	ret;
}

	// .globl	reduce_min_u32
.visible .entry reduce_min_u32(
	.param .u64 reduce_min_u32_param_0,
	.param .u32 reduce_min_u32_param_1,
	.param .u64 reduce_min_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_min_u32_param_0];
	ld.param.u32 	%r21, [reduce_min_u32_param_1];
	ld.param.u64 	%rd2, [reduce_min_u32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, -1;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB13_4;

BB13_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	min.u32 	%r62, %r62, %r27;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB13_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	min.u32 	%r62, %r62, %r28;

BB13_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB13_1;

BB13_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB13_6;

	ld.shared.u32 	%r33, [%r11+2048];
	min.u32 	%r62, %r62, %r33;
	st.shared.u32 	[%r11], %r62;

BB13_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB13_8;

	ld.shared.u32 	%r35, [%r11+1024];
	min.u32 	%r62, %r62, %r35;
	st.shared.u32 	[%r11], %r62;

BB13_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB13_10;

	ld.shared.u32 	%r37, [%r11+512];
	min.u32 	%r62, %r62, %r37;
	st.shared.u32 	[%r11], %r62;

BB13_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB13_12;

	ld.shared.u32 	%r39, [%r11+256];
	min.u32 	%r62, %r62, %r39;
	st.shared.u32 	[%r11], %r62;

BB13_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB13_15;

	ld.shared.u32 	%r41, [%r11+128];
	min.u32 	%r42, %r62, %r41;
	mov.u32 	%r43, 2;
	mov.u32 	%r44, 31;
	mov.u32 	%r45, 16;
	mov.u32 	%r46, -1;
	shfl.sync.down.b32 	%r47|%p9, %r42, %r45, %r44, %r46;
	min.u32 	%r48, %r42, %r47;
	mov.u32 	%r49, 8;
	shfl.sync.down.b32 	%r50|%p10, %r48, %r49, %r44, %r46;
	min.u32 	%r51, %r48, %r50;
	mov.u32 	%r52, 4;
	shfl.sync.down.b32 	%r53|%p11, %r51, %r52, %r44, %r46;
	min.u32 	%r54, %r51, %r53;
	shfl.sync.down.b32 	%r55|%p12, %r54, %r43, %r44, %r46;
	min.u32 	%r56, %r54, %r55;
	mov.u32 	%r57, 1;
	shfl.sync.down.b32 	%r58|%p13, %r56, %r57, %r44, %r46;
	min.u32 	%r20, %r56, %r58;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB13_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB13_15:
	ret;
}

	// .globl	reduce_min_i64
.visible .entry reduce_min_i64(
	.param .u64 reduce_min_i64_param_0,
	.param .u32 reduce_min_i64_param_1,
	.param .u64 reduce_min_i64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd19, [reduce_min_i64_param_0];
	ld.param.u32 	%r9, [reduce_min_i64_param_1];
	ld.param.u64 	%rd16, [reduce_min_i64_param_2];
	cvta.to.global.u64 	%rd1, %rd19;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.u64 	%rd44, 9223372036854775807;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB14_4;

BB14_1:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	min.s64 	%rd44, %rd44, %rd22;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB14_3;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	min.s64 	%rd44, %rd44, %rd25;

BB14_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB14_1;

BB14_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB14_6;

	ld.shared.u64 	%rd26, [%r8+4096];
	min.s64 	%rd44, %rd44, %rd26;
	st.shared.u64 	[%r8], %rd44;

BB14_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB14_8;

	ld.shared.u64 	%rd27, [%r8+2048];
	min.s64 	%rd44, %rd44, %rd27;
	st.shared.u64 	[%r8], %rd44;

BB14_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB14_10;

	ld.shared.u64 	%rd28, [%r8+1024];
	min.s64 	%rd44, %rd44, %rd28;
	st.shared.u64 	[%r8], %rd44;

BB14_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB14_12;

	ld.shared.u64 	%rd29, [%r8+512];
	min.s64 	%rd44, %rd44, %rd29;
	st.shared.u64 	[%r8], %rd44;

BB14_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB14_15;

	ld.shared.u64 	%rd40, [%r8+256];
	min.s64 	%rd30, %rd44, %rd40;
	// inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// inline asm
	mov.b64 %rd31, {%r16,%r17};
	// inline asm
	min.s64 	%rd32, %rd30, %rd31;
	// inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// inline asm
	mov.b64 %rd33, {%r20,%r21};
	// inline asm
	min.s64 	%rd34, %rd32, %rd33;
	// inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// inline asm
	mov.b64 %rd35, {%r24,%r25};
	// inline asm
	min.s64 	%rd36, %rd34, %rd35;
	// inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// inline asm
	mov.b64 %rd37, {%r28,%r29};
	// inline asm
	min.s64 	%rd38, %rd36, %rd37;
	// inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// inline asm
	mov.b64 %rd39, {%r32,%r33};
	// inline asm
	min.s64 	%rd15, %rd38, %rd39;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB14_15;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

BB14_15:
	ret;
}

	// .globl	reduce_min_u64
.visible .entry reduce_min_u64(
	.param .u64 reduce_min_u64_param_0,
	.param .u32 reduce_min_u64_param_1,
	.param .u64 reduce_min_u64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd19, [reduce_min_u64_param_0];
	ld.param.u32 	%r9, [reduce_min_u64_param_1];
	ld.param.u64 	%rd16, [reduce_min_u64_param_2];
	cvta.to.global.u64 	%rd1, %rd19;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.u64 	%rd44, -1;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB15_4;

BB15_1:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	min.u64 	%rd44, %rd44, %rd22;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB15_3;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	min.u64 	%rd44, %rd44, %rd25;

BB15_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB15_1;

BB15_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB15_6;

	ld.shared.u64 	%rd26, [%r8+4096];
	min.u64 	%rd44, %rd44, %rd26;
	st.shared.u64 	[%r8], %rd44;

BB15_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB15_8;

	ld.shared.u64 	%rd27, [%r8+2048];
	min.u64 	%rd44, %rd44, %rd27;
	st.shared.u64 	[%r8], %rd44;

BB15_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB15_10;

	ld.shared.u64 	%rd28, [%r8+1024];
	min.u64 	%rd44, %rd44, %rd28;
	st.shared.u64 	[%r8], %rd44;

BB15_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB15_12;

	ld.shared.u64 	%rd29, [%r8+512];
	min.u64 	%rd44, %rd44, %rd29;
	st.shared.u64 	[%r8], %rd44;

BB15_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB15_15;

	ld.shared.u64 	%rd40, [%r8+256];
	min.u64 	%rd30, %rd44, %rd40;
	// inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// inline asm
	mov.b64 %rd31, {%r16,%r17};
	// inline asm
	min.u64 	%rd32, %rd30, %rd31;
	// inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// inline asm
	mov.b64 %rd33, {%r20,%r21};
	// inline asm
	min.u64 	%rd34, %rd32, %rd33;
	// inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// inline asm
	mov.b64 %rd35, {%r24,%r25};
	// inline asm
	min.u64 	%rd36, %rd34, %rd35;
	// inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// inline asm
	mov.b64 %rd37, {%r28,%r29};
	// inline asm
	min.u64 	%rd38, %rd36, %rd37;
	// inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// inline asm
	mov.b64 %rd39, {%r32,%r33};
	// inline asm
	min.u64 	%rd15, %rd38, %rd39;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB15_15;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

BB15_15:
	ret;
}

	// .globl	reduce_min_f32
.visible .entry reduce_min_f32(
	.param .u64 reduce_min_f32_param_0,
	.param .u32 reduce_min_f32_param_1,
	.param .u64 reduce_min_f32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<41>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_min_f32_param_0];
	ld.param.u32 	%r7, [reduce_min_f32_param_1];
	ld.param.u64 	%rd2, [reduce_min_f32_param_2];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	add.s32 	%r38, %r10, %r8;
	mov.f32 	%f34, 0f7F800000;
	setp.ge.u32	%p1, %r38, %r7;
	@%p1 bra 	BB16_4;

BB16_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r38, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f17, [%rd5];
	min.f32 	%f34, %f34, %f17;
	add.s32 	%r3, %r38, 1024;
	setp.ge.u32	%p2, %r3, %r7;
	@%p2 bra 	BB16_3;

	mul.wide.u32 	%rd7, %r3, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.f32 	%f18, [%rd8];
	min.f32 	%f34, %f34, %f18;

BB16_3:
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r12, %r11, 11;
	add.s32 	%r38, %r38, %r12;
	setp.lt.u32	%p3, %r38, %r7;
	@%p3 bra 	BB16_1;

BB16_4:
	shl.b32 	%r13, %r8, 2;
	mov.u32 	%r14, shared;
	add.s32 	%r6, %r14, %r13;
	st.shared.f32 	[%r6], %f34;
	bar.sync 	0;
	setp.gt.u32	%p4, %r8, 511;
	@%p4 bra 	BB16_6;

	ld.shared.f32 	%f19, [%r6+2048];
	min.f32 	%f34, %f34, %f19;
	st.shared.f32 	[%r6], %f34;

BB16_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r8, 255;
	@%p5 bra 	BB16_8;

	ld.shared.f32 	%f20, [%r6+1024];
	min.f32 	%f34, %f34, %f20;
	st.shared.f32 	[%r6], %f34;

BB16_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r8, 127;
	@%p6 bra 	BB16_10;

	ld.shared.f32 	%f21, [%r6+512];
	min.f32 	%f34, %f34, %f21;
	st.shared.f32 	[%r6], %f34;

BB16_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r8, 63;
	@%p7 bra 	BB16_12;

	ld.shared.f32 	%f22, [%r6+256];
	min.f32 	%f34, %f34, %f22;
	st.shared.f32 	[%r6], %f34;

BB16_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r8, 31;
	@%p8 bra 	BB16_15;

	ld.shared.f32 	%f23, [%r6+128];
	min.f32 	%f24, %f34, %f23;
	mov.b32 	 %r19, %f24;
	mov.u32 	%r20, 2;
	mov.u32 	%r21, 31;
	mov.u32 	%r22, 16;
	mov.u32 	%r23, -1;
	shfl.sync.down.b32 	%r24|%p9, %r19, %r22, %r21, %r23;
	mov.b32 	 %f25, %r24;
	min.f32 	%f26, %f24, %f25;
	mov.b32 	 %r25, %f26;
	mov.u32 	%r26, 8;
	shfl.sync.down.b32 	%r27|%p10, %r25, %r26, %r21, %r23;
	mov.b32 	 %f27, %r27;
	min.f32 	%f28, %f26, %f27;
	mov.b32 	 %r28, %f28;
	mov.u32 	%r29, 4;
	shfl.sync.down.b32 	%r30|%p11, %r28, %r29, %r21, %r23;
	mov.b32 	 %f29, %r30;
	min.f32 	%f30, %f28, %f29;
	mov.b32 	 %r31, %f30;
	shfl.sync.down.b32 	%r32|%p12, %r31, %r20, %r21, %r23;
	mov.b32 	 %f31, %r32;
	min.f32 	%f32, %f30, %f31;
	mov.b32 	 %r33, %f32;
	mov.u32 	%r34, 1;
	shfl.sync.down.b32 	%r35|%p13, %r33, %r34, %r21, %r23;
	mov.b32 	 %f33, %r35;
	min.f32 	%f14, %f32, %f33;
	setp.ne.s32	%p14, %r8, 0;
	@%p14 bra 	BB16_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r9, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.f32 	[%rd11], %f14;

BB16_15:
	ret;
}

	// .globl	reduce_min_f64
.visible .entry reduce_min_f64(
	.param .u64 reduce_min_f64_param_0,
	.param .u32 reduce_min_f64_param_1,
	.param .u64 reduce_min_f64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<41>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_min_f64_param_0];
	ld.param.u32 	%r9, [reduce_min_f64_param_1];
	ld.param.u64 	%rd2, [reduce_min_f64_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.f64 	%fd34, 0d7FF0000000000000;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB17_4;

BB17_1:
	mul.wide.u32 	%rd4, %r41, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f64 	%fd17, [%rd5];
	min.f64 	%fd34, %fd34, %fd17;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB17_3;

	mul.wide.u32 	%rd6, %r6, 8;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f64 	%fd18, [%rd7];
	min.f64 	%fd34, %fd34, %fd18;

BB17_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB17_1;

BB17_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared_d;
	add.s32 	%r8, %r13, %r12;
	st.shared.f64 	[%r8], %fd34;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB17_6;

	ld.shared.f64 	%fd19, [%r8+4096];
	min.f64 	%fd34, %fd34, %fd19;
	st.shared.f64 	[%r8], %fd34;

BB17_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB17_8;

	ld.shared.f64 	%fd20, [%r8+2048];
	min.f64 	%fd34, %fd34, %fd20;
	st.shared.f64 	[%r8], %fd34;

BB17_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB17_10;

	ld.shared.f64 	%fd21, [%r8+1024];
	min.f64 	%fd34, %fd34, %fd21;
	st.shared.f64 	[%r8], %fd34;

BB17_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB17_12;

	ld.shared.f64 	%fd22, [%r8+512];
	min.f64 	%fd34, %fd34, %fd22;
	st.shared.f64 	[%r8], %fd34;

BB17_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB17_15;

	ld.shared.f64 	%fd33, [%r8+256];
	min.f64 	%fd23, %fd34, %fd33;
	// inline asm
	mov.b64 {%r14,%r15}, %fd23;
	// inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// inline asm
	mov.b64 %fd24, {%r16,%r17};
	// inline asm
	min.f64 	%fd25, %fd23, %fd24;
	// inline asm
	mov.b64 {%r18,%r19}, %fd25;
	// inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// inline asm
	mov.b64 %fd26, {%r20,%r21};
	// inline asm
	min.f64 	%fd27, %fd25, %fd26;
	// inline asm
	mov.b64 {%r22,%r23}, %fd27;
	// inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// inline asm
	mov.b64 %fd28, {%r24,%r25};
	// inline asm
	min.f64 	%fd29, %fd27, %fd28;
	// inline asm
	mov.b64 {%r26,%r27}, %fd29;
	// inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// inline asm
	mov.b64 %fd30, {%r28,%r29};
	// inline asm
	min.f64 	%fd31, %fd29, %fd30;
	// inline asm
	mov.b64 {%r30,%r31}, %fd31;
	// inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// inline asm
	mov.b64 %fd32, {%r32,%r33};
	// inline asm
	min.f64 	%fd14, %fd31, %fd32;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB17_15;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r1, 8;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f64 	[%rd10], %fd14;

BB17_15:
	ret;
}

	// .globl	reduce_max_i32
.visible .entry reduce_max_i32(
	.param .u64 reduce_max_i32_param_0,
	.param .u32 reduce_max_i32_param_1,
	.param .u64 reduce_max_i32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_max_i32_param_0];
	ld.param.u32 	%r21, [reduce_max_i32_param_1];
	ld.param.u64 	%rd2, [reduce_max_i32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, -2147483648;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB18_4;

BB18_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	max.s32 	%r62, %r62, %r27;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB18_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	max.s32 	%r62, %r62, %r28;

BB18_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB18_1;

BB18_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB18_6;

	ld.shared.u32 	%r33, [%r11+2048];
	max.s32 	%r62, %r62, %r33;
	st.shared.u32 	[%r11], %r62;

BB18_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB18_8;

	ld.shared.u32 	%r35, [%r11+1024];
	max.s32 	%r62, %r62, %r35;
	st.shared.u32 	[%r11], %r62;

BB18_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB18_10;

	ld.shared.u32 	%r37, [%r11+512];
	max.s32 	%r62, %r62, %r37;
	st.shared.u32 	[%r11], %r62;

BB18_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB18_12;

	ld.shared.u32 	%r39, [%r11+256];
	max.s32 	%r62, %r62, %r39;
	st.shared.u32 	[%r11], %r62;

BB18_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB18_15;

	ld.shared.u32 	%r41, [%r11+128];
	max.s32 	%r42, %r62, %r41;
	mov.u32 	%r43, 2;
	mov.u32 	%r44, 31;
	mov.u32 	%r45, 16;
	mov.u32 	%r46, -1;
	shfl.sync.down.b32 	%r47|%p9, %r42, %r45, %r44, %r46;
	max.s32 	%r48, %r42, %r47;
	mov.u32 	%r49, 8;
	shfl.sync.down.b32 	%r50|%p10, %r48, %r49, %r44, %r46;
	max.s32 	%r51, %r48, %r50;
	mov.u32 	%r52, 4;
	shfl.sync.down.b32 	%r53|%p11, %r51, %r52, %r44, %r46;
	max.s32 	%r54, %r51, %r53;
	shfl.sync.down.b32 	%r55|%p12, %r54, %r43, %r44, %r46;
	max.s32 	%r56, %r54, %r55;
	mov.u32 	%r57, 1;
	shfl.sync.down.b32 	%r58|%p13, %r56, %r57, %r44, %r46;
	max.s32 	%r20, %r56, %r58;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB18_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB18_15:
	ret;
}

	// .globl	reduce_max_u32
.visible .entry reduce_max_u32(
	.param .u64 reduce_max_u32_param_0,
	.param .u32 reduce_max_u32_param_1,
	.param .u64 reduce_max_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_max_u32_param_0];
	ld.param.u32 	%r21, [reduce_max_u32_param_1];
	ld.param.u64 	%rd2, [reduce_max_u32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, 0;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB19_4;

BB19_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	max.u32 	%r62, %r62, %r27;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB19_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	max.u32 	%r62, %r62, %r28;

BB19_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB19_1;

BB19_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB19_6;

	ld.shared.u32 	%r33, [%r11+2048];
	max.u32 	%r62, %r62, %r33;
	st.shared.u32 	[%r11], %r62;

BB19_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB19_8;

	ld.shared.u32 	%r35, [%r11+1024];
	max.u32 	%r62, %r62, %r35;
	st.shared.u32 	[%r11], %r62;

BB19_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB19_10;

	ld.shared.u32 	%r37, [%r11+512];
	max.u32 	%r62, %r62, %r37;
	st.shared.u32 	[%r11], %r62;

BB19_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB19_12;

	ld.shared.u32 	%r39, [%r11+256];
	max.u32 	%r62, %r62, %r39;
	st.shared.u32 	[%r11], %r62;

BB19_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB19_15;

	ld.shared.u32 	%r41, [%r11+128];
	max.u32 	%r42, %r62, %r41;
	mov.u32 	%r43, 2;
	mov.u32 	%r44, 31;
	mov.u32 	%r45, 16;
	mov.u32 	%r46, -1;
	shfl.sync.down.b32 	%r47|%p9, %r42, %r45, %r44, %r46;
	max.u32 	%r48, %r42, %r47;
	mov.u32 	%r49, 8;
	shfl.sync.down.b32 	%r50|%p10, %r48, %r49, %r44, %r46;
	max.u32 	%r51, %r48, %r50;
	mov.u32 	%r52, 4;
	shfl.sync.down.b32 	%r53|%p11, %r51, %r52, %r44, %r46;
	max.u32 	%r54, %r51, %r53;
	shfl.sync.down.b32 	%r55|%p12, %r54, %r43, %r44, %r46;
	max.u32 	%r56, %r54, %r55;
	mov.u32 	%r57, 1;
	shfl.sync.down.b32 	%r58|%p13, %r56, %r57, %r44, %r46;
	max.u32 	%r20, %r56, %r58;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB19_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB19_15:
	ret;
}

	// .globl	reduce_max_i64
.visible .entry reduce_max_i64(
	.param .u64 reduce_max_i64_param_0,
	.param .u32 reduce_max_i64_param_1,
	.param .u64 reduce_max_i64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd19, [reduce_max_i64_param_0];
	ld.param.u32 	%r9, [reduce_max_i64_param_1];
	ld.param.u64 	%rd16, [reduce_max_i64_param_2];
	cvta.to.global.u64 	%rd1, %rd19;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.u64 	%rd44, -9223372036854775808;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB20_4;

BB20_1:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	max.s64 	%rd44, %rd44, %rd22;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB20_3;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	max.s64 	%rd44, %rd44, %rd25;

BB20_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB20_1;

BB20_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB20_6;

	ld.shared.u64 	%rd26, [%r8+4096];
	max.s64 	%rd44, %rd44, %rd26;
	st.shared.u64 	[%r8], %rd44;

BB20_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB20_8;

	ld.shared.u64 	%rd27, [%r8+2048];
	max.s64 	%rd44, %rd44, %rd27;
	st.shared.u64 	[%r8], %rd44;

BB20_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB20_10;

	ld.shared.u64 	%rd28, [%r8+1024];
	max.s64 	%rd44, %rd44, %rd28;
	st.shared.u64 	[%r8], %rd44;

BB20_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB20_12;

	ld.shared.u64 	%rd29, [%r8+512];
	max.s64 	%rd44, %rd44, %rd29;
	st.shared.u64 	[%r8], %rd44;

BB20_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB20_15;

	ld.shared.u64 	%rd40, [%r8+256];
	max.s64 	%rd30, %rd44, %rd40;
	// inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// inline asm
	mov.b64 %rd31, {%r16,%r17};
	// inline asm
	max.s64 	%rd32, %rd30, %rd31;
	// inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// inline asm
	mov.b64 %rd33, {%r20,%r21};
	// inline asm
	max.s64 	%rd34, %rd32, %rd33;
	// inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// inline asm
	mov.b64 %rd35, {%r24,%r25};
	// inline asm
	max.s64 	%rd36, %rd34, %rd35;
	// inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// inline asm
	mov.b64 %rd37, {%r28,%r29};
	// inline asm
	max.s64 	%rd38, %rd36, %rd37;
	// inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// inline asm
	mov.b64 %rd39, {%r32,%r33};
	// inline asm
	max.s64 	%rd15, %rd38, %rd39;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB20_15;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

BB20_15:
	ret;
}

	// .globl	reduce_max_u64
.visible .entry reduce_max_u64(
	.param .u64 reduce_max_u64_param_0,
	.param .u32 reduce_max_u64_param_1,
	.param .u64 reduce_max_u64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd19, [reduce_max_u64_param_0];
	ld.param.u32 	%r9, [reduce_max_u64_param_1];
	ld.param.u64 	%rd16, [reduce_max_u64_param_2];
	cvta.to.global.u64 	%rd1, %rd19;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.u64 	%rd44, 0;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB21_4;

BB21_1:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	max.u64 	%rd44, %rd44, %rd22;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB21_3;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	max.u64 	%rd44, %rd44, %rd25;

BB21_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB21_1;

BB21_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB21_6;

	ld.shared.u64 	%rd26, [%r8+4096];
	max.u64 	%rd44, %rd44, %rd26;
	st.shared.u64 	[%r8], %rd44;

BB21_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB21_8;

	ld.shared.u64 	%rd27, [%r8+2048];
	max.u64 	%rd44, %rd44, %rd27;
	st.shared.u64 	[%r8], %rd44;

BB21_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB21_10;

	ld.shared.u64 	%rd28, [%r8+1024];
	max.u64 	%rd44, %rd44, %rd28;
	st.shared.u64 	[%r8], %rd44;

BB21_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB21_12;

	ld.shared.u64 	%rd29, [%r8+512];
	max.u64 	%rd44, %rd44, %rd29;
	st.shared.u64 	[%r8], %rd44;

BB21_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB21_15;

	ld.shared.u64 	%rd40, [%r8+256];
	max.u64 	%rd30, %rd44, %rd40;
	// inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// inline asm
	mov.b64 %rd31, {%r16,%r17};
	// inline asm
	max.u64 	%rd32, %rd30, %rd31;
	// inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// inline asm
	mov.b64 %rd33, {%r20,%r21};
	// inline asm
	max.u64 	%rd34, %rd32, %rd33;
	// inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// inline asm
	mov.b64 %rd35, {%r24,%r25};
	// inline asm
	max.u64 	%rd36, %rd34, %rd35;
	// inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// inline asm
	mov.b64 %rd37, {%r28,%r29};
	// inline asm
	max.u64 	%rd38, %rd36, %rd37;
	// inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// inline asm
	mov.b64 %rd39, {%r32,%r33};
	// inline asm
	max.u64 	%rd15, %rd38, %rd39;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB21_15;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

BB21_15:
	ret;
}

	// .globl	reduce_max_f32
.visible .entry reduce_max_f32(
	.param .u64 reduce_max_f32_param_0,
	.param .u32 reduce_max_f32_param_1,
	.param .u64 reduce_max_f32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<41>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_max_f32_param_0];
	ld.param.u32 	%r7, [reduce_max_f32_param_1];
	ld.param.u64 	%rd2, [reduce_max_f32_param_2];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	add.s32 	%r38, %r10, %r8;
	mov.f32 	%f34, 0fFF800000;
	setp.ge.u32	%p1, %r38, %r7;
	@%p1 bra 	BB22_4;

BB22_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r38, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f17, [%rd5];
	max.f32 	%f34, %f34, %f17;
	add.s32 	%r3, %r38, 1024;
	setp.ge.u32	%p2, %r3, %r7;
	@%p2 bra 	BB22_3;

	mul.wide.u32 	%rd7, %r3, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.f32 	%f18, [%rd8];
	max.f32 	%f34, %f34, %f18;

BB22_3:
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r12, %r11, 11;
	add.s32 	%r38, %r38, %r12;
	setp.lt.u32	%p3, %r38, %r7;
	@%p3 bra 	BB22_1;

BB22_4:
	shl.b32 	%r13, %r8, 2;
	mov.u32 	%r14, shared;
	add.s32 	%r6, %r14, %r13;
	st.shared.f32 	[%r6], %f34;
	bar.sync 	0;
	setp.gt.u32	%p4, %r8, 511;
	@%p4 bra 	BB22_6;

	ld.shared.f32 	%f19, [%r6+2048];
	max.f32 	%f34, %f34, %f19;
	st.shared.f32 	[%r6], %f34;

BB22_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r8, 255;
	@%p5 bra 	BB22_8;

	ld.shared.f32 	%f20, [%r6+1024];
	max.f32 	%f34, %f34, %f20;
	st.shared.f32 	[%r6], %f34;

BB22_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r8, 127;
	@%p6 bra 	BB22_10;

	ld.shared.f32 	%f21, [%r6+512];
	max.f32 	%f34, %f34, %f21;
	st.shared.f32 	[%r6], %f34;

BB22_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r8, 63;
	@%p7 bra 	BB22_12;

	ld.shared.f32 	%f22, [%r6+256];
	max.f32 	%f34, %f34, %f22;
	st.shared.f32 	[%r6], %f34;

BB22_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r8, 31;
	@%p8 bra 	BB22_15;

	ld.shared.f32 	%f23, [%r6+128];
	max.f32 	%f24, %f34, %f23;
	mov.b32 	 %r19, %f24;
	mov.u32 	%r20, 2;
	mov.u32 	%r21, 31;
	mov.u32 	%r22, 16;
	mov.u32 	%r23, -1;
	shfl.sync.down.b32 	%r24|%p9, %r19, %r22, %r21, %r23;
	mov.b32 	 %f25, %r24;
	max.f32 	%f26, %f24, %f25;
	mov.b32 	 %r25, %f26;
	mov.u32 	%r26, 8;
	shfl.sync.down.b32 	%r27|%p10, %r25, %r26, %r21, %r23;
	mov.b32 	 %f27, %r27;
	max.f32 	%f28, %f26, %f27;
	mov.b32 	 %r28, %f28;
	mov.u32 	%r29, 4;
	shfl.sync.down.b32 	%r30|%p11, %r28, %r29, %r21, %r23;
	mov.b32 	 %f29, %r30;
	max.f32 	%f30, %f28, %f29;
	mov.b32 	 %r31, %f30;
	shfl.sync.down.b32 	%r32|%p12, %r31, %r20, %r21, %r23;
	mov.b32 	 %f31, %r32;
	max.f32 	%f32, %f30, %f31;
	mov.b32 	 %r33, %f32;
	mov.u32 	%r34, 1;
	shfl.sync.down.b32 	%r35|%p13, %r33, %r34, %r21, %r23;
	mov.b32 	 %f33, %r35;
	max.f32 	%f14, %f32, %f33;
	setp.ne.s32	%p14, %r8, 0;
	@%p14 bra 	BB22_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r9, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.f32 	[%rd11], %f14;

BB22_15:
	ret;
}

	// .globl	reduce_max_f64
.visible .entry reduce_max_f64(
	.param .u64 reduce_max_f64_param_0,
	.param .u32 reduce_max_f64_param_1,
	.param .u64 reduce_max_f64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<41>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_max_f64_param_0];
	ld.param.u32 	%r9, [reduce_max_f64_param_1];
	ld.param.u64 	%rd2, [reduce_max_f64_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.f64 	%fd34, 0dFFF0000000000000;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB23_4;

BB23_1:
	mul.wide.u32 	%rd4, %r41, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f64 	%fd17, [%rd5];
	max.f64 	%fd34, %fd34, %fd17;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB23_3;

	mul.wide.u32 	%rd6, %r6, 8;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f64 	%fd18, [%rd7];
	max.f64 	%fd34, %fd34, %fd18;

BB23_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB23_1;

BB23_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared_d;
	add.s32 	%r8, %r13, %r12;
	st.shared.f64 	[%r8], %fd34;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB23_6;

	ld.shared.f64 	%fd19, [%r8+4096];
	max.f64 	%fd34, %fd34, %fd19;
	st.shared.f64 	[%r8], %fd34;

BB23_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB23_8;

	ld.shared.f64 	%fd20, [%r8+2048];
	max.f64 	%fd34, %fd34, %fd20;
	st.shared.f64 	[%r8], %fd34;

BB23_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB23_10;

	ld.shared.f64 	%fd21, [%r8+1024];
	max.f64 	%fd34, %fd34, %fd21;
	st.shared.f64 	[%r8], %fd34;

BB23_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB23_12;

	ld.shared.f64 	%fd22, [%r8+512];
	max.f64 	%fd34, %fd34, %fd22;
	st.shared.f64 	[%r8], %fd34;

BB23_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB23_15;

	ld.shared.f64 	%fd33, [%r8+256];
	max.f64 	%fd23, %fd34, %fd33;
	// inline asm
	mov.b64 {%r14,%r15}, %fd23;
	// inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// inline asm
	mov.b64 %fd24, {%r16,%r17};
	// inline asm
	max.f64 	%fd25, %fd23, %fd24;
	// inline asm
	mov.b64 {%r18,%r19}, %fd25;
	// inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// inline asm
	mov.b64 %fd26, {%r20,%r21};
	// inline asm
	max.f64 	%fd27, %fd25, %fd26;
	// inline asm
	mov.b64 {%r22,%r23}, %fd27;
	// inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// inline asm
	mov.b64 %fd28, {%r24,%r25};
	// inline asm
	max.f64 	%fd29, %fd27, %fd28;
	// inline asm
	mov.b64 {%r26,%r27}, %fd29;
	// inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// inline asm
	mov.b64 %fd30, {%r28,%r29};
	// inline asm
	max.f64 	%fd31, %fd29, %fd30;
	// inline asm
	mov.b64 {%r30,%r31}, %fd31;
	// inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// inline asm
	mov.b64 %fd32, {%r32,%r33};
	// inline asm
	max.f64 	%fd14, %fd31, %fd32;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB23_15;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r1, 8;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f64 	[%rd10], %fd14;

BB23_15:
	ret;
}

	// .globl	reduce_or_u32
.visible .entry reduce_or_u32(
	.param .u64 reduce_or_u32_param_0,
	.param .u32 reduce_or_u32_param_1,
	.param .u64 reduce_or_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_or_u32_param_0];
	ld.param.u32 	%r21, [reduce_or_u32_param_1];
	ld.param.u64 	%rd2, [reduce_or_u32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, 0;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB24_4;

BB24_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	or.b32  	%r62, %r27, %r62;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB24_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	or.b32  	%r62, %r28, %r62;

BB24_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB24_1;

BB24_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB24_6;

	ld.shared.u32 	%r33, [%r11+2048];
	or.b32  	%r62, %r33, %r62;
	st.shared.u32 	[%r11], %r62;

BB24_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB24_8;

	ld.shared.u32 	%r35, [%r11+1024];
	or.b32  	%r62, %r35, %r62;
	st.shared.u32 	[%r11], %r62;

BB24_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB24_10;

	ld.shared.u32 	%r37, [%r11+512];
	or.b32  	%r62, %r37, %r62;
	st.shared.u32 	[%r11], %r62;

BB24_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB24_12;

	ld.shared.u32 	%r39, [%r11+256];
	or.b32  	%r62, %r39, %r62;
	st.shared.u32 	[%r11], %r62;

BB24_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB24_15;

	ld.shared.u32 	%r41, [%r11+128];
	or.b32  	%r42, %r41, %r62;
	mov.u32 	%r43, 2;
	mov.u32 	%r44, 31;
	mov.u32 	%r45, 16;
	mov.u32 	%r46, -1;
	shfl.sync.down.b32 	%r47|%p9, %r42, %r45, %r44, %r46;
	or.b32  	%r48, %r47, %r42;
	mov.u32 	%r49, 8;
	shfl.sync.down.b32 	%r50|%p10, %r48, %r49, %r44, %r46;
	or.b32  	%r51, %r50, %r48;
	mov.u32 	%r52, 4;
	shfl.sync.down.b32 	%r53|%p11, %r51, %r52, %r44, %r46;
	or.b32  	%r54, %r53, %r51;
	shfl.sync.down.b32 	%r55|%p12, %r54, %r43, %r44, %r46;
	or.b32  	%r56, %r55, %r54;
	mov.u32 	%r57, 1;
	shfl.sync.down.b32 	%r58|%p13, %r56, %r57, %r44, %r46;
	or.b32  	%r20, %r58, %r56;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB24_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB24_15:
	ret;
}

	// .globl	reduce_and_u32
.visible .entry reduce_and_u32(
	.param .u64 reduce_and_u32_param_0,
	.param .u32 reduce_and_u32_param_1,
	.param .u64 reduce_and_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_and_u32_param_0];
	ld.param.u32 	%r21, [reduce_and_u32_param_1];
	ld.param.u64 	%rd2, [reduce_and_u32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, -1;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB25_4;

BB25_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	and.b32  	%r62, %r27, %r62;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB25_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	and.b32  	%r62, %r28, %r62;

BB25_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB25_1;

BB25_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB25_6;

	ld.shared.u32 	%r33, [%r11+2048];
	and.b32  	%r62, %r33, %r62;
	st.shared.u32 	[%r11], %r62;

BB25_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB25_8;

	ld.shared.u32 	%r35, [%r11+1024];
	and.b32  	%r62, %r35, %r62;
	st.shared.u32 	[%r11], %r62;

BB25_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB25_10;

	ld.shared.u32 	%r37, [%r11+512];
	and.b32  	%r62, %r37, %r62;
	st.shared.u32 	[%r11], %r62;

BB25_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB25_12;

	ld.shared.u32 	%r39, [%r11+256];
	and.b32  	%r62, %r39, %r62;
	st.shared.u32 	[%r11], %r62;

BB25_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB25_15;

	ld.shared.u32 	%r41, [%r11+128];
	and.b32  	%r42, %r41, %r62;
	mov.u32 	%r43, 2;
	mov.u32 	%r44, 31;
	mov.u32 	%r45, 16;
	mov.u32 	%r46, -1;
	shfl.sync.down.b32 	%r47|%p9, %r42, %r45, %r44, %r46;
	and.b32  	%r48, %r47, %r42;
	mov.u32 	%r49, 8;
	shfl.sync.down.b32 	%r50|%p10, %r48, %r49, %r44, %r46;
	and.b32  	%r51, %r50, %r48;
	mov.u32 	%r52, 4;
	shfl.sync.down.b32 	%r53|%p11, %r51, %r52, %r44, %r46;
	and.b32  	%r54, %r53, %r51;
	shfl.sync.down.b32 	%r55|%p12, %r54, %r43, %r44, %r46;
	and.b32  	%r56, %r55, %r54;
	mov.u32 	%r57, 1;
	shfl.sync.down.b32 	%r58|%p13, %r56, %r57, %r44, %r46;
	and.b32  	%r20, %r58, %r56;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB25_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB25_15:
	ret;
}

	// .globl	scan_small_u32
.visible .entry scan_small_u32(
	.param .u64 scan_small_u32_param_0,
	.param .u64 scan_small_u32_param_1,
	.param .u32 scan_small_u32_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd2, [scan_small_u32_param_0];
	ld.param.u64 	%rd1, [scan_small_u32_param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	mov.u32 	%r12, %tid.x;
	mul.wide.u32 	%rd4, %r12, 16;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.v4.u32 	{%r13, %r14, %r15, %r16}, [%rd5];
	mov.u32 	%r17, 0;
	mov.u32 	%r40, 1;
	add.s32 	%r2, %r14, %r13;
	add.s32 	%r3, %r15, %r2;
	add.s32 	%r4, %r16, %r3;
	shl.b32 	%r21, %r12, 2;
	mov.u32 	%r22, shared;
	add.s32 	%r23, %r22, %r21;
	st.shared.u32 	[%r23], %r17;
	mov.u32 	%r24, %ntid.x;
	add.s32 	%r25, %r24, %r12;
	shl.b32 	%r26, %r25, 2;
	add.s32 	%r5, %r22, %r26;
	st.shared.u32 	[%r5], %r4;
	setp.lt.u32	%p1, %r24, 2;
	mov.u32 	%r41, %r4;
	@%p1 bra 	BB26_2;

BB26_1:
	bar.sync 	0;
	sub.s32 	%r29, %r25, %r40;
	shl.b32 	%r30, %r29, 2;
	add.s32 	%r32, %r22, %r30;
	ld.shared.u32 	%r33, [%r32];
	ld.shared.u32 	%r34, [%r5];
	add.s32 	%r41, %r33, %r34;
	bar.sync 	0;
	st.shared.u32 	[%r5], %r41;
	shl.b32 	%r40, %r40, 1;
	setp.lt.u32	%p2, %r40, %r24;
	@%p2 bra 	BB26_1;

BB26_2:
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd8, %rd6, %rd4;
	sub.s32 	%r36, %r41, %r4;
	add.s32 	%r37, %r36, %r3;
	add.s32 	%r38, %r36, %r2;
	add.s32 	%r39, %r36, %r13;
	st.global.v4.u32 	[%rd8], {%r36, %r39, %r38, %r37};
	ret;
}

	// .globl	scan_large_u32_init
.visible .entry scan_large_u32_init(
	.param .u64 scan_large_u32_init_param_0,
	.param .u32 scan_large_u32_init_param_1
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd2, [scan_large_u32_init_param_0];
	ld.param.u32 	%r6, [scan_large_u32_init_param_1];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.u32	%p1, %r10, %r6;
	@%p1 bra 	BB27_3;

	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;
	cvta.to.global.u64 	%rd1, %rd2;

BB27_2:
	setp.lt.u32	%p2, %r10, 32;
	selp.b64	%rd3, 2, 0, %p2;
	mul.wide.u32 	%rd4, %r10, 8;
	add.s64 	%rd5, %rd1, %rd4;
	st.global.u64 	[%rd5], %rd3;
	add.s32 	%r10, %r3, %r10;
	setp.lt.u32	%p3, %r10, %r6;
	@%p3 bra 	BB27_2;

BB27_3:
	ret;
}

	// .globl	scan_large_u32
.visible .entry scan_large_u32(
	.param .u64 scan_large_u32_param_0,
	.param .u64 scan_large_u32_param_1,
	.param .u64 scan_large_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<204>;
	.reg .b64 	%rd<29>;


	ld.param.u64 	%rd7, [scan_large_u32_param_0];
	ld.param.u64 	%rd5, [scan_large_u32_param_1];
	ld.param.u64 	%rd6, [scan_large_u32_param_2];
	cvta.to.global.u64 	%rd8, %rd7;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r40, %r1, 9;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r40, %r2;
	cvt.u64.u32	%rd1, %r41;
	mul.wide.u32 	%rd9, %r41, 16;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v4.u32 	{%r42, %r43, %r44, %r45}, [%rd10];
	ld.global.v4.u32 	{%r50, %r51, %r52, %r53}, [%rd10+2048];
	ld.global.v4.u32 	{%r58, %r59, %r60, %r61}, [%rd10+4096];
	ld.global.v4.u32 	{%r66, %r67, %r68, %r69}, [%rd10+6144];
	shl.b32 	%r74, %r2, 4;
	mov.u32 	%r75, shared;
	add.s32 	%r3, %r75, %r74;
	st.shared.v4.u32 	[%r3], {%r42, %r43, %r44, %r45};
	st.shared.v4.u32 	[%r3+2048], {%r50, %r51, %r52, %r53};
	st.shared.v4.u32 	[%r3+4096], {%r58, %r59, %r60, %r61};
	st.shared.v4.u32 	[%r3+6144], {%r66, %r67, %r68, %r69};
	bar.sync 	0;
	shl.b32 	%r76, %r2, 6;
	add.s32 	%r4, %r75, %r76;
	ld.shared.v4.u32 	{%r78, %r79, %r80, %r81}, [%r4];
	ld.shared.v4.u32 	{%r85, %r86, %r87, %r88}, [%r4+16];
	ld.shared.v4.u32 	{%r93, %r94, %r95, %r96}, [%r4+32];
	ld.shared.v4.u32 	{%r101, %r102, %r103, %r104}, [%r4+48];
	add.s32 	%r6, %r79, %r78;
	add.s32 	%r7, %r80, %r6;
	add.s32 	%r8, %r81, %r7;
	add.s32 	%r9, %r85, %r8;
	add.s32 	%r10, %r86, %r9;
	add.s32 	%r11, %r87, %r10;
	add.s32 	%r12, %r88, %r11;
	add.s32 	%r13, %r93, %r12;
	add.s32 	%r14, %r94, %r13;
	add.s32 	%r15, %r95, %r14;
	add.s32 	%r16, %r96, %r15;
	add.s32 	%r17, %r101, %r16;
	add.s32 	%r18, %r102, %r17;
	add.s32 	%r19, %r103, %r18;
	add.s32 	%r20, %r104, %r19;
	bar.sync 	0;
	shl.b32 	%r109, %r2, 2;
	add.s32 	%r21, %r75, %r109;
	mov.u32 	%r203, 0;
	st.shared.u32 	[%r21], %r203;
	st.shared.u32 	[%r21+512], %r20;
	bar.sync 	0;
	ld.shared.u32 	%r112, [%r21+508];
	ld.shared.u32 	%r113, [%r21+512];
	add.s32 	%r22, %r112, %r113;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r22;
	bar.sync 	0;
	ld.shared.u32 	%r114, [%r21+504];
	ld.shared.u32 	%r115, [%r21+512];
	add.s32 	%r23, %r114, %r115;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r23;
	bar.sync 	0;
	ld.shared.u32 	%r116, [%r21+496];
	ld.shared.u32 	%r117, [%r21+512];
	add.s32 	%r24, %r116, %r117;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r24;
	bar.sync 	0;
	ld.shared.u32 	%r118, [%r21+480];
	ld.shared.u32 	%r119, [%r21+512];
	add.s32 	%r25, %r118, %r119;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r25;
	bar.sync 	0;
	ld.shared.u32 	%r120, [%r21+448];
	ld.shared.u32 	%r121, [%r21+512];
	add.s32 	%r26, %r120, %r121;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r26;
	bar.sync 	0;
	ld.shared.u32 	%r122, [%r21+384];
	ld.shared.u32 	%r123, [%r21+512];
	add.s32 	%r27, %r122, %r123;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r27;
	bar.sync 	0;
	ld.shared.u32 	%r124, [%r21+256];
	ld.shared.u32 	%r125, [%r21+512];
	add.s32 	%r28, %r124, %r125;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r28;
	cvt.u64.u32	%rd2, %r1;
	mul.wide.u32 	%rd11, %r1, 8;
	add.s64 	%rd3, %rd6, %rd11;
	setp.ne.s32	%p1, %r2, 127;
	@%p1 bra 	BB28_2;

	cvt.u64.u32	%rd14, %r28;
	shl.b64 	%rd15, %rd14, 32;
	or.b64  	%rd13, %rd15, 1;
	// inline asm
	st.cg.u64 [%rd3], %rd13;
	// inline asm

BB28_2:
	mov.u32 	%r29, WARP_SZ;
	add.s32 	%r127, %r29, -1;
	and.b32  	%r30, %r127, %r2;
	sub.s32 	%r202, %r30, %r29;
	bra.uni 	BB28_3;

BB28_8:
	add.s32 	%r203, %r35, %r203;
	sub.s32 	%r202, %r202, %r29;

BB28_3:
	cvt.s64.s32	%rd18, %r202;
	add.s64 	%rd19, %rd18, %rd2;
	shl.b64 	%rd20, %rd19, 3;
	add.s64 	%rd17, %rd6, %rd20;
	// inline asm
	ld.cg.u64 %rd16, [%rd17];
	// inline asm
	cvt.u32.u64	%r34, %rd16;
	setp.eq.s32	%p2, %r34, 0;
	mov.u32 	%r128, -1;
	vote.sync.any.pred 	%p3, %p2, %r128;
	@%p3 bra 	BB28_3;

	setp.eq.s32	%p4, %r34, 2;
	vote.sync.ballot.b32 	%r36, %p4, %r128;
	shr.u64 	%rd21, %rd16, 32;
	cvt.u32.u64	%r35, %rd21;
	setp.eq.s32	%p6, %r36, 0;
	@%p6 bra 	BB28_8;

	clz.b32 	%r131, %r36;
	mov.u32 	%r132, 31;
	sub.s32 	%r133, %r132, %r131;
	setp.lt.u32	%p7, %r30, %r133;
	selp.b32	%r134, 0, %r35, %p7;
	mov.u32 	%r135, 0;
	add.s32 	%r136, %r134, %r203;
	mov.u32 	%r137, 2;
	mov.u32 	%r138, 16;
	shfl.sync.down.b32 	%r140|%p8, %r136, %r138, %r132, %r128;
	add.s32 	%r141, %r140, %r136;
	mov.u32 	%r142, 8;
	shfl.sync.down.b32 	%r143|%p9, %r141, %r142, %r132, %r128;
	add.s32 	%r144, %r143, %r141;
	mov.u32 	%r145, 4;
	shfl.sync.down.b32 	%r146|%p10, %r144, %r145, %r132, %r128;
	add.s32 	%r147, %r146, %r144;
	shfl.sync.down.b32 	%r148|%p11, %r147, %r137, %r132, %r128;
	add.s32 	%r149, %r148, %r147;
	mov.u32 	%r150, 1;
	shfl.sync.down.b32 	%r151|%p12, %r149, %r150, %r132, %r128;
	add.s32 	%r152, %r151, %r149;
	shfl.sync.idx.b32 	%r153|%p13, %r152, %r135, %r132, %r128;
	add.s32 	%r37, %r153, %r28;
	@%p1 bra 	BB28_7;

	cvt.u64.u32	%rd24, %r37;
	shl.b64 	%rd25, %rd24, 32;
	or.b64  	%rd23, %rd25, 2;
	// inline asm
	st.cg.u64 [%rd3], %rd23;
	// inline asm

BB28_7:
	sub.s32 	%r154, %r37, %r20;
	add.s32 	%r155, %r154, %r7;
	add.s32 	%r156, %r154, %r6;
	add.s32 	%r157, %r154, %r78;
	st.shared.v4.u32 	[%r4], {%r154, %r157, %r156, %r155};
	add.s32 	%r158, %r154, %r11;
	add.s32 	%r159, %r154, %r10;
	add.s32 	%r160, %r154, %r9;
	add.s32 	%r161, %r154, %r8;
	st.shared.v4.u32 	[%r4+16], {%r161, %r160, %r159, %r158};
	add.s32 	%r162, %r154, %r15;
	add.s32 	%r163, %r154, %r14;
	add.s32 	%r164, %r154, %r13;
	add.s32 	%r165, %r154, %r12;
	st.shared.v4.u32 	[%r4+32], {%r165, %r164, %r163, %r162};
	add.s32 	%r166, %r154, %r19;
	add.s32 	%r167, %r154, %r18;
	add.s32 	%r168, %r154, %r17;
	add.s32 	%r169, %r154, %r16;
	st.shared.v4.u32 	[%r4+48], {%r169, %r168, %r167, %r166};
	bar.sync 	0;
	cvta.to.global.u64 	%rd26, %rd5;
	shl.b64 	%rd27, %rd1, 4;
	add.s64 	%rd28, %rd26, %rd27;
	ld.shared.v4.u32 	{%r170, %r171, %r172, %r173}, [%r3];
	st.global.v4.u32 	[%rd28], {%r170, %r171, %r172, %r173};
	ld.shared.v4.u32 	{%r178, %r179, %r180, %r181}, [%r3+2048];
	st.global.v4.u32 	[%rd28+2048], {%r178, %r179, %r180, %r181};
	ld.shared.v4.u32 	{%r186, %r187, %r188, %r189}, [%r3+4096];
	st.global.v4.u32 	[%rd28+4096], {%r186, %r187, %r188, %r189};
	ld.shared.v4.u32 	{%r194, %r195, %r196, %r197}, [%r3+6144];
	st.global.v4.u32 	[%rd28+6144], {%r194, %r195, %r196, %r197};
	ret;
}

	// .globl	compress_small
.visible .entry compress_small(
	.param .u64 compress_small_param_0,
	.param .u64 compress_small_param_1,
	.param .u32 compress_small_param_2,
	.param .u64 compress_small_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<54>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd3, [compress_small_param_0];
	ld.param.u64 	%rd1, [compress_small_param_1];
	ld.param.u64 	%rd2, [compress_small_param_3];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r16, %tid.x;
	mul.wide.u32 	%rd5, %r16, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u32 	%r17, [%rd6];
	and.b32  	%r1, %r17, 255;
	bfe.u32 	%r18, %r17, 8, 8;
	add.s32 	%r2, %r18, %r1;
	bfe.u32 	%r19, %r17, 16, 8;
	add.s32 	%r3, %r19, %r2;
	shr.u32 	%r20, %r17, 24;
	add.s32 	%r4, %r20, %r3;
	shl.b32 	%r21, %r16, 2;
	mov.u32 	%r22, shared;
	add.s32 	%r23, %r22, %r21;
	mov.u32 	%r24, 0;
	st.shared.u32 	[%r23], %r24;
	mov.u32 	%r25, %ntid.x;
	add.s32 	%r26, %r25, %r16;
	shl.b32 	%r27, %r26, 2;
	add.s32 	%r28, %r22, %r27;
	st.shared.u32 	[%r28], %r4;
	mov.u32 	%r52, 1;
	setp.lt.u32	%p1, %r25, 2;
	mov.u32 	%r53, %r4;
	@%p1 bra 	BB29_2;

BB29_1:
	bar.sync 	0;
	sub.s32 	%r33, %r26, %r52;
	shl.b32 	%r34, %r33, 2;
	add.s32 	%r35, %r22, %r34;
	ld.shared.u32 	%r36, [%r35];
	ld.shared.u32 	%r37, [%r28];
	add.s32 	%r53, %r36, %r37;
	bar.sync 	0;
	st.shared.u32 	[%r28], %r53;
	shl.b32 	%r52, %r52, 1;
	setp.lt.u32	%p2, %r52, %r25;
	@%p2 bra 	BB29_1;

BB29_2:
	add.s32 	%r39, %r25, -1;
	setp.ne.s32	%p3, %r16, %r39;
	@%p3 bra 	BB29_4;

	cvta.to.global.u64 	%rd7, %rd2;
	st.global.u32 	[%rd7], %r53;

BB29_4:
	sub.s32 	%r11, %r53, %r4;
	add.s32 	%r12, %r11, %r1;
	add.s32 	%r13, %r11, %r2;
	add.s32 	%r14, %r11, %r3;
	setp.eq.s32	%p4, %r1, 0;
	@%p4 bra 	BB29_6;

	cvta.to.global.u64 	%rd8, %rd1;
	mul.wide.u32 	%rd9, %r11, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u32 	[%rd10], %r21;

BB29_6:
	setp.eq.s32	%p5, %r12, %r13;
	@%p5 bra 	BB29_8;

	add.s32 	%r45, %r21, 1;
	cvta.to.global.u64 	%rd11, %rd1;
	mul.wide.u32 	%rd12, %r12, 4;
	add.s64 	%rd13, %rd11, %rd12;
	st.global.u32 	[%rd13], %r45;

BB29_8:
	setp.eq.s32	%p6, %r13, %r14;
	@%p6 bra 	BB29_10;

	add.s32 	%r48, %r21, 2;
	cvta.to.global.u64 	%rd14, %rd1;
	mul.wide.u32 	%rd15, %r13, 4;
	add.s64 	%rd16, %rd14, %rd15;
	st.global.u32 	[%rd16], %r48;

BB29_10:
	setp.eq.s32	%p7, %r14, %r53;
	@%p7 bra 	BB29_12;

	add.s32 	%r51, %r21, 3;
	cvta.to.global.u64 	%rd17, %rd1;
	mul.wide.u32 	%rd18, %r14, 4;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.u32 	[%rd19], %r51;

BB29_12:
	ret;
}

	// .globl	compress_large
.visible .entry compress_large(
	.param .u64 compress_large_param_0,
	.param .u64 compress_large_param_1,
	.param .u64 compress_large_param_2,
	.param .u64 compress_large_param_3
)
{
	.reg .pred 	%p<32>;
	.reg .b32 	%r<145>;
	.reg .b64 	%rd<60>;


	ld.param.u64 	%rd8, [compress_large_param_0];
	ld.param.u64 	%rd9, [compress_large_param_1];
	ld.param.u64 	%rd6, [compress_large_param_2];
	ld.param.u64 	%rd7, [compress_large_param_3];
	cvta.to.global.u64 	%rd1, %rd9;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r56, %r1, 7;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r3, %r56, %r2;
	cvta.to.global.u64 	%rd10, %rd8;
	mul.wide.u32 	%rd11, %r3, 16;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.v4.u32 	{%r57, %r58, %r59, %r60}, [%rd12];
	mov.u32 	%r143, 0;
	and.b32  	%r4, %r57, 255;
	bfe.u32 	%r66, %r57, 8, 8;
	add.s32 	%r5, %r66, %r4;
	bfe.u32 	%r67, %r57, 16, 8;
	add.s32 	%r6, %r67, %r5;
	shr.u32 	%r68, %r57, 24;
	add.s32 	%r7, %r68, %r6;
	and.b32  	%r69, %r58, 255;
	add.s32 	%r8, %r69, %r7;
	bfe.u32 	%r70, %r58, 8, 8;
	add.s32 	%r9, %r70, %r8;
	bfe.u32 	%r71, %r58, 16, 8;
	add.s32 	%r10, %r71, %r9;
	shr.u32 	%r72, %r58, 24;
	add.s32 	%r11, %r72, %r10;
	and.b32  	%r73, %r59, 255;
	add.s32 	%r12, %r73, %r11;
	bfe.u32 	%r74, %r59, 8, 8;
	add.s32 	%r13, %r74, %r12;
	bfe.u32 	%r75, %r59, 16, 8;
	add.s32 	%r14, %r75, %r13;
	shr.u32 	%r76, %r59, 24;
	add.s32 	%r15, %r76, %r14;
	and.b32  	%r77, %r60, 255;
	add.s32 	%r16, %r77, %r15;
	bfe.u32 	%r78, %r60, 8, 8;
	add.s32 	%r17, %r78, %r16;
	bfe.u32 	%r79, %r60, 16, 8;
	add.s32 	%r18, %r79, %r17;
	shr.u32 	%r80, %r60, 24;
	add.s32 	%r19, %r80, %r18;
	shl.b32 	%r81, %r2, 2;
	mov.u32 	%r82, shared;
	add.s32 	%r20, %r82, %r81;
	st.shared.u32 	[%r20], %r143;
	st.shared.u32 	[%r20+512], %r19;
	bar.sync 	0;
	ld.shared.u32 	%r83, [%r20+508];
	ld.shared.u32 	%r84, [%r20+512];
	add.s32 	%r21, %r83, %r84;
	bar.sync 	0;
	st.shared.u32 	[%r20+512], %r21;
	bar.sync 	0;
	ld.shared.u32 	%r85, [%r20+504];
	ld.shared.u32 	%r86, [%r20+512];
	add.s32 	%r22, %r85, %r86;
	bar.sync 	0;
	st.shared.u32 	[%r20+512], %r22;
	bar.sync 	0;
	ld.shared.u32 	%r87, [%r20+496];
	ld.shared.u32 	%r88, [%r20+512];
	add.s32 	%r23, %r87, %r88;
	bar.sync 	0;
	st.shared.u32 	[%r20+512], %r23;
	bar.sync 	0;
	ld.shared.u32 	%r89, [%r20+480];
	ld.shared.u32 	%r90, [%r20+512];
	add.s32 	%r24, %r89, %r90;
	bar.sync 	0;
	st.shared.u32 	[%r20+512], %r24;
	bar.sync 	0;
	ld.shared.u32 	%r91, [%r20+448];
	ld.shared.u32 	%r92, [%r20+512];
	add.s32 	%r25, %r91, %r92;
	bar.sync 	0;
	st.shared.u32 	[%r20+512], %r25;
	bar.sync 	0;
	ld.shared.u32 	%r93, [%r20+384];
	ld.shared.u32 	%r94, [%r20+512];
	add.s32 	%r26, %r93, %r94;
	bar.sync 	0;
	st.shared.u32 	[%r20+512], %r26;
	bar.sync 	0;
	ld.shared.u32 	%r95, [%r20+256];
	ld.shared.u32 	%r96, [%r20+512];
	add.s32 	%r27, %r95, %r96;
	bar.sync 	0;
	st.shared.u32 	[%r20+512], %r27;
	cvt.u64.u32	%rd2, %r1;
	mul.wide.u32 	%rd13, %r1, 8;
	add.s64 	%rd3, %rd6, %rd13;
	setp.ne.s32	%p1, %r2, 127;
	@%p1 bra 	BB30_2;

	cvt.u64.u32	%rd16, %r27;
	shl.b64 	%rd17, %rd16, 32;
	or.b64  	%rd15, %rd17, 1;
	// inline asm
	st.cg.u64 [%rd3], %rd15;
	// inline asm

BB30_2:
	mov.u32 	%r28, WARP_SZ;
	add.s32 	%r98, %r28, -1;
	and.b32  	%r29, %r98, %r2;
	sub.s32 	%r144, %r29, %r28;
	cvta.to.global.u64 	%rd4, %rd7;
	bra.uni 	BB30_3;

BB30_41:
	add.s32 	%r143, %r34, %r143;
	sub.s32 	%r144, %r144, %r28;

BB30_3:
	cvt.s64.s32	%rd20, %r144;
	add.s64 	%rd21, %rd20, %rd2;
	shl.b64 	%rd22, %rd21, 3;
	add.s64 	%rd19, %rd6, %rd22;
	// inline asm
	ld.cg.u64 %rd18, [%rd19];
	// inline asm
	cvt.u32.u64	%r33, %rd18;
	setp.eq.s32	%p2, %r33, 0;
	mov.u32 	%r99, -1;
	vote.sync.any.pred 	%p3, %p2, %r99;
	@%p3 bra 	BB30_3;

	setp.eq.s32	%p4, %r33, 2;
	vote.sync.ballot.b32 	%r35, %p4, %r99;
	shr.u64 	%rd23, %rd18, 32;
	cvt.u32.u64	%r34, %rd23;
	setp.eq.s32	%p6, %r35, 0;
	@%p6 bra 	BB30_41;

	clz.b32 	%r102, %r35;
	mov.u32 	%r103, 31;
	sub.s32 	%r104, %r103, %r102;
	setp.lt.u32	%p7, %r29, %r104;
	selp.b32	%r105, 0, %r34, %p7;
	mov.u32 	%r106, 0;
	add.s32 	%r107, %r105, %r143;
	mov.u32 	%r108, 2;
	mov.u32 	%r109, 16;
	shfl.sync.down.b32 	%r111|%p8, %r107, %r109, %r103, %r99;
	add.s32 	%r112, %r111, %r107;
	mov.u32 	%r113, 8;
	shfl.sync.down.b32 	%r114|%p9, %r112, %r113, %r103, %r99;
	add.s32 	%r115, %r114, %r112;
	mov.u32 	%r116, 4;
	shfl.sync.down.b32 	%r117|%p10, %r115, %r116, %r103, %r99;
	add.s32 	%r118, %r117, %r115;
	shfl.sync.down.b32 	%r119|%p11, %r118, %r108, %r103, %r99;
	add.s32 	%r120, %r119, %r118;
	mov.u32 	%r121, 1;
	shfl.sync.down.b32 	%r122|%p12, %r120, %r121, %r103, %r99;
	add.s32 	%r123, %r122, %r120;
	shfl.sync.idx.b32 	%r124|%p13, %r123, %r106, %r103, %r99;
	add.s32 	%r36, %r124, %r27;
	@%p1 bra 	BB30_8;

	cvt.u32.u64	%r125, %rd2;
	cvt.u64.u32	%rd26, %r36;
	shl.b64 	%rd27, %rd26, 32;
	or.b64  	%rd25, %rd27, 2;
	// inline asm
	st.cg.u64 [%rd3], %rd25;
	// inline asm
	mov.u32 	%r126, %nctaid.x;
	add.s32 	%r127, %r126, -1;
	setp.ne.s32	%p15, %r125, %r127;
	@%p15 bra 	BB30_8;

	st.global.u32 	[%rd4], %r36;

BB30_8:
	sub.s32 	%r37, %r36, %r19;
	add.s32 	%r38, %r37, %r4;
	add.s32 	%r39, %r37, %r5;
	add.s32 	%r40, %r37, %r6;
	add.s32 	%r41, %r37, %r7;
	add.s32 	%r42, %r37, %r8;
	add.s32 	%r43, %r37, %r9;
	add.s32 	%r44, %r37, %r10;
	add.s32 	%r45, %r37, %r11;
	add.s32 	%r46, %r37, %r12;
	add.s32 	%r47, %r37, %r13;
	add.s32 	%r48, %r37, %r14;
	add.s32 	%r49, %r37, %r15;
	add.s32 	%r50, %r37, %r16;
	add.s32 	%r51, %r37, %r17;
	add.s32 	%r52, %r37, %r18;
	shl.b32 	%r53, %r3, 4;
	setp.eq.s32	%p16, %r4, 0;
	@%p16 bra 	BB30_10;

	mul.wide.u32 	%rd28, %r37, 4;
	add.s64 	%rd29, %rd1, %rd28;
	st.global.u32 	[%rd29], %r53;

BB30_10:
	setp.eq.s32	%p17, %r38, %r39;
	@%p17 bra 	BB30_12;

	add.s32 	%r128, %r53, 1;
	mul.wide.u32 	%rd30, %r38, 4;
	add.s64 	%rd31, %rd1, %rd30;
	st.global.u32 	[%rd31], %r128;

BB30_12:
	setp.eq.s32	%p18, %r39, %r40;
	@%p18 bra 	BB30_14;

	add.s32 	%r129, %r53, 2;
	mul.wide.u32 	%rd32, %r39, 4;
	add.s64 	%rd33, %rd1, %rd32;
	st.global.u32 	[%rd33], %r129;

BB30_14:
	setp.eq.s32	%p19, %r40, %r41;
	@%p19 bra 	BB30_16;

	add.s32 	%r130, %r53, 3;
	mul.wide.u32 	%rd34, %r40, 4;
	add.s64 	%rd35, %rd1, %rd34;
	st.global.u32 	[%rd35], %r130;

BB30_16:
	setp.eq.s32	%p20, %r41, %r42;
	@%p20 bra 	BB30_18;

	add.s32 	%r131, %r53, 4;
	mul.wide.u32 	%rd36, %r41, 4;
	add.s64 	%rd37, %rd1, %rd36;
	st.global.u32 	[%rd37], %r131;

BB30_18:
	setp.eq.s32	%p21, %r42, %r43;
	@%p21 bra 	BB30_20;

	add.s32 	%r132, %r53, 5;
	mul.wide.u32 	%rd38, %r42, 4;
	add.s64 	%rd39, %rd1, %rd38;
	st.global.u32 	[%rd39], %r132;

BB30_20:
	setp.eq.s32	%p22, %r43, %r44;
	@%p22 bra 	BB30_22;

	add.s32 	%r133, %r53, 6;
	mul.wide.u32 	%rd40, %r43, 4;
	add.s64 	%rd41, %rd1, %rd40;
	st.global.u32 	[%rd41], %r133;

BB30_22:
	setp.eq.s32	%p23, %r44, %r45;
	@%p23 bra 	BB30_24;

	add.s32 	%r134, %r53, 7;
	mul.wide.u32 	%rd42, %r44, 4;
	add.s64 	%rd43, %rd1, %rd42;
	st.global.u32 	[%rd43], %r134;

BB30_24:
	setp.eq.s32	%p24, %r45, %r46;
	@%p24 bra 	BB30_26;

	add.s32 	%r135, %r53, 8;
	mul.wide.u32 	%rd44, %r45, 4;
	add.s64 	%rd45, %rd1, %rd44;
	st.global.u32 	[%rd45], %r135;

BB30_26:
	setp.eq.s32	%p25, %r46, %r47;
	@%p25 bra 	BB30_28;

	add.s32 	%r136, %r53, 9;
	mul.wide.u32 	%rd46, %r46, 4;
	add.s64 	%rd47, %rd1, %rd46;
	st.global.u32 	[%rd47], %r136;

BB30_28:
	setp.eq.s32	%p26, %r47, %r48;
	@%p26 bra 	BB30_30;

	add.s32 	%r137, %r53, 10;
	mul.wide.u32 	%rd48, %r47, 4;
	add.s64 	%rd49, %rd1, %rd48;
	st.global.u32 	[%rd49], %r137;

BB30_30:
	setp.eq.s32	%p27, %r48, %r49;
	@%p27 bra 	BB30_32;

	add.s32 	%r138, %r53, 11;
	mul.wide.u32 	%rd50, %r48, 4;
	add.s64 	%rd51, %rd1, %rd50;
	st.global.u32 	[%rd51], %r138;

BB30_32:
	setp.eq.s32	%p28, %r49, %r50;
	@%p28 bra 	BB30_34;

	add.s32 	%r139, %r53, 12;
	mul.wide.u32 	%rd52, %r49, 4;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.u32 	[%rd53], %r139;

BB30_34:
	setp.eq.s32	%p29, %r50, %r51;
	@%p29 bra 	BB30_36;

	add.s32 	%r140, %r53, 13;
	mul.wide.u32 	%rd54, %r50, 4;
	add.s64 	%rd55, %rd1, %rd54;
	st.global.u32 	[%rd55], %r140;

BB30_36:
	setp.eq.s32	%p30, %r51, %r52;
	@%p30 bra 	BB30_38;

	add.s32 	%r141, %r53, 14;
	mul.wide.u32 	%rd56, %r51, 4;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.u32 	[%rd57], %r141;

BB30_38:
	setp.eq.s32	%p31, %r52, %r36;
	@%p31 bra 	BB30_40;

	add.s32 	%r142, %r53, 15;
	mul.wide.u32 	%rd58, %r52, 4;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.u32 	[%rd59], %r142;

BB30_40:
	ret;
}

	// .globl	mkperm_phase_1_tiny
.visible .entry mkperm_phase_1_tiny(
	.param .u64 mkperm_phase_1_tiny_param_0,
	.param .u64 mkperm_phase_1_tiny_param_1,
	.param .u32 mkperm_phase_1_tiny_param_2,
	.param .u32 mkperm_phase_1_tiny_param_3,
	.param .u32 mkperm_phase_1_tiny_param_4
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<60>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd3, [mkperm_phase_1_tiny_param_0];
	ld.param.u64 	%rd4, [mkperm_phase_1_tiny_param_1];
	ld.param.u32 	%r22, [mkperm_phase_1_tiny_param_2];
	ld.param.u32 	%r24, [mkperm_phase_1_tiny_param_3];
	ld.param.u32 	%r23, [mkperm_phase_1_tiny_param_4];
	mov.u32 	%r25, %ctaid.x;
	mul.lo.s32 	%r1, %r25, %r24;
	add.s32 	%r2, %r1, %r24;
	mov.u32 	%r26, WARP_SZ;
	mov.u32 	%r3, %ntid.x;
	div.u32 	%r27, %r3, %r26;
	mov.u32 	%r4, %tid.x;
	div.u32 	%r5, %r4, %r26;
	mul.lo.s32 	%r6, %r27, %r23;
	setp.ge.u32	%p3, %r4, %r6;
	@%p3 bra 	BB31_3;

	mov.u32 	%r56, %r4;

BB31_2:
	shl.b32 	%r28, %r56, 2;
	mov.u32 	%r29, shared;
	add.s32 	%r30, %r29, %r28;
	mov.u32 	%r31, 0;
	st.shared.u32 	[%r30], %r31;
	add.s32 	%r56, %r56, %r3;
	setp.lt.u32	%p4, %r56, %r6;
	@%p4 bra 	BB31_2;

BB31_3:
	bar.sync 	0;
	add.s32 	%r57, %r1, %r4;
	setp.ge.u32	%p5, %r57, %r2;
	@%p5 bra 	BB31_10;

	mul.lo.s32 	%r10, %r5, %r23;
	cvta.to.global.u64 	%rd5, %rd3;

BB31_5:
	setp.lt.u32	%p6, %r57, %r22;
	mov.u32 	%r32, -1;
	vote.sync.ballot.b32 	%r12, %p6, %r32;
	setp.ge.u32	%p7, %r57, %r22;
	@%p7 bra 	BB31_9;

	mul.wide.u32 	%rd6, %r57, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u32 	%r13, [%rd7];
	match.any.sync.b32 	%r14, %r13, %r12;
	neg.s32 	%r34, %r14;
	mov.u32 	%r58, 0;
	and.b32  	%r35, %r14, %r34;
	clz.b32 	%r36, %r35;
	mov.u32 	%r37, 31;
	sub.s32 	%r15, %r37, %r36;
	add.s32 	%r39, %r26, -1;
	mov.u32 	%r40, %tid.x;
	and.b32  	%r41, %r39, %r40;
	setp.ne.s32	%p8, %r41, %r15;
	@%p8 bra 	BB31_8;

	add.s32 	%r42, %r13, %r10;
	shl.b32 	%r43, %r42, 2;
	mov.u32 	%r44, shared;
	add.s32 	%r45, %r44, %r43;
	popc.b32 	%r46, %r14;
	ld.shared.u32 	%r58, [%r45];
	add.s32 	%r47, %r46, %r58;
	st.shared.u32 	[%r45], %r47;

BB31_8:
	shfl.sync.idx.b32 	%r49|%p9, %r58, %r15, %r37, %r14;

BB31_9:
	bar.warp.sync 	-1;
	add.s32 	%r57, %r57, %r3;
	setp.lt.u32	%p10, %r57, %r2;
	@%p10 bra 	BB31_5;

BB31_10:
	mov.u32 	%r59, %tid.x;
	setp.lt.u32	%p2, %r59, %r6;
	bar.sync 	0;
	mul.lo.s32 	%r51, %r6, %r25;
	cvt.u64.u32	%rd1, %r51;
	cvta.to.global.u64 	%rd2, %rd4;
	@!%p2 bra 	BB31_12;
	bra.uni 	BB31_11;

BB31_11:
	shl.b32 	%r52, %r59, 2;
	mov.u32 	%r53, shared;
	add.s32 	%r54, %r53, %r52;
	ld.shared.u32 	%r55, [%r54];
	cvt.u64.u32	%rd8, %r59;
	add.s64 	%rd9, %rd8, %rd1;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd2, %rd10;
	st.global.u32 	[%rd11], %r55;
	add.s32 	%r59, %r59, %r3;
	setp.lt.u32	%p11, %r59, %r6;
	@%p11 bra 	BB31_11;

BB31_12:
	ret;
}

	// .globl	mkperm_phase_1_small
.visible .entry mkperm_phase_1_small(
	.param .u64 mkperm_phase_1_small_param_0,
	.param .u64 mkperm_phase_1_small_param_1,
	.param .u32 mkperm_phase_1_small_param_2,
	.param .u32 mkperm_phase_1_small_param_3,
	.param .u32 mkperm_phase_1_small_param_4
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<52>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd4, [mkperm_phase_1_small_param_0];
	ld.param.u64 	%rd5, [mkperm_phase_1_small_param_1];
	ld.param.u32 	%r20, [mkperm_phase_1_small_param_2];
	ld.param.u32 	%r22, [mkperm_phase_1_small_param_3];
	ld.param.u32 	%r21, [mkperm_phase_1_small_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mul.lo.s32 	%r2, %r23, %r22;
	add.s32 	%r3, %r2, %r22;
	mov.u32 	%r4, %tid.x;
	setp.ge.u32	%p3, %r4, %r21;
	@%p3 bra 	BB32_3;

	mov.u32 	%r48, %r4;

BB32_2:
	shl.b32 	%r24, %r48, 2;
	mov.u32 	%r25, shared;
	add.s32 	%r26, %r25, %r24;
	mov.u32 	%r27, 0;
	st.shared.u32 	[%r26], %r27;
	add.s32 	%r48, %r48, %r1;
	setp.lt.u32	%p4, %r48, %r21;
	@%p4 bra 	BB32_2;

BB32_3:
	bar.sync 	0;
	add.s32 	%r49, %r2, %r4;
	setp.ge.u32	%p5, %r49, %r3;
	@%p5 bra 	BB32_10;

	mov.u32 	%r28, WARP_SZ;
	add.s32 	%r29, %r28, -1;
	and.b32  	%r8, %r29, %r4;
	cvta.to.global.u64 	%rd1, %rd4;

BB32_5:
	setp.lt.u32	%p6, %r49, %r20;
	mov.u32 	%r30, -1;
	vote.sync.ballot.b32 	%r10, %p6, %r30;
	setp.ge.u32	%p7, %r49, %r20;
	@%p7 bra 	BB32_9;

	mul.wide.u32 	%rd6, %r49, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u32 	%r11, [%rd7];
	match.any.sync.b32 	%r12, %r11, %r10;
	neg.s32 	%r32, %r12;
	mov.u32 	%r50, 0;
	and.b32  	%r33, %r12, %r32;
	clz.b32 	%r34, %r33;
	mov.u32 	%r35, 31;
	sub.s32 	%r13, %r35, %r34;
	setp.ne.s32	%p8, %r8, %r13;
	@%p8 bra 	BB32_8;

	popc.b32 	%r36, %r12;
	shl.b32 	%r37, %r11, 2;
	mov.u32 	%r38, shared;
	add.s32 	%r39, %r38, %r37;
	atom.shared.add.u32 	%r50, [%r39], %r36;

BB32_8:
	shfl.sync.idx.b32 	%r41|%p9, %r50, %r13, %r35, %r12;

BB32_9:
	add.s32 	%r49, %r49, %r1;
	setp.lt.u32	%p10, %r49, %r3;
	@%p10 bra 	BB32_5;

BB32_10:
	mov.u32 	%r51, %tid.x;
	setp.lt.u32	%p2, %r51, %r21;
	bar.sync 	0;
	mul.lo.s32 	%r43, %r23, %r21;
	cvt.u64.u32	%rd2, %r43;
	cvta.to.global.u64 	%rd3, %rd5;
	@!%p2 bra 	BB32_12;
	bra.uni 	BB32_11;

BB32_11:
	shl.b32 	%r44, %r51, 2;
	mov.u32 	%r45, shared;
	add.s32 	%r46, %r45, %r44;
	ld.shared.u32 	%r47, [%r46];
	cvt.u64.u32	%rd8, %r51;
	add.s64 	%rd9, %rd8, %rd2;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd3, %rd10;
	st.global.u32 	[%rd11], %r47;
	add.s32 	%r51, %r51, %r1;
	setp.lt.u32	%p11, %r51, %r21;
	@%p11 bra 	BB32_11;

BB32_12:
	ret;
}

	// .globl	mkperm_phase_1_large
.visible .entry mkperm_phase_1_large(
	.param .u64 mkperm_phase_1_large_param_0,
	.param .u64 mkperm_phase_1_large_param_1,
	.param .u32 mkperm_phase_1_large_param_2,
	.param .u32 mkperm_phase_1_large_param_3,
	.param .u32 mkperm_phase_1_large_param_4
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<34>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd3, [mkperm_phase_1_large_param_0];
	ld.param.u64 	%rd4, [mkperm_phase_1_large_param_1];
	ld.param.u32 	%r13, [mkperm_phase_1_large_param_2];
	ld.param.u32 	%r15, [mkperm_phase_1_large_param_3];
	ld.param.u32 	%r14, [mkperm_phase_1_large_param_4];
	mov.u32 	%r1, %ctaid.x;
	mul.lo.s32 	%r16, %r1, %r15;
	add.s32 	%r2, %r16, %r15;
	mov.u32 	%r17, %tid.x;
	add.s32 	%r32, %r16, %r17;
	setp.ge.u32	%p2, %r32, %r2;
	@%p2 bra 	BB33_7;

	mul.lo.s32 	%r18, %r1, %r14;
	cvt.u64.u32	%rd1, %r18;
	mov.u32 	%r4, %ntid.x;
	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd5, %rd3;

BB33_2:
	setp.lt.u32	%p3, %r32, %r13;
	mov.u32 	%r19, -1;
	vote.sync.ballot.b32 	%r6, %p3, %r19;
	setp.ge.u32	%p4, %r32, %r13;
	@%p4 bra 	BB33_6;

	mul.wide.u32 	%rd6, %r32, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u32 	%r7, [%rd7];
	match.any.sync.b32 	%r8, %r7, %r6;
	neg.s32 	%r21, %r8;
	mov.u32 	%r33, 0;
	and.b32  	%r22, %r8, %r21;
	clz.b32 	%r23, %r22;
	mov.u32 	%r24, 31;
	sub.s32 	%r9, %r24, %r23;
	mov.u32 	%r25, WARP_SZ;
	add.s32 	%r26, %r25, -1;
	and.b32  	%r28, %r26, %r17;
	setp.ne.s32	%p5, %r28, %r9;
	@%p5 bra 	BB33_5;

	popc.b32 	%r29, %r8;
	cvt.u64.u32	%rd8, %r7;
	add.s64 	%rd9, %rd8, %rd1;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd2, %rd10;
	atom.global.add.u32 	%r33, [%rd11], %r29;

BB33_5:
	shfl.sync.idx.b32 	%r31|%p6, %r33, %r9, %r24, %r8;

BB33_6:
	add.s32 	%r32, %r32, %r4;
	setp.lt.u32	%p7, %r32, %r2;
	@%p7 bra 	BB33_2;

BB33_7:
	ret;
}

	// .globl	mkperm_phase_3
.visible .entry mkperm_phase_3(
	.param .u64 mkperm_phase_3_param_0,
	.param .u32 mkperm_phase_3_param_1,
	.param .u32 mkperm_phase_3_param_2,
	.param .u32 mkperm_phase_3_param_3,
	.param .u64 mkperm_phase_3_param_4,
	.param .u64 mkperm_phase_3_param_5
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd1, [mkperm_phase_3_param_0];
	ld.param.u32 	%r20, [mkperm_phase_3_param_1];
	ld.param.u32 	%r21, [mkperm_phase_3_param_2];
	ld.param.u32 	%r22, [mkperm_phase_3_param_3];
	ld.param.u64 	%rd2, [mkperm_phase_3_param_4];
	ld.param.u64 	%rd3, [mkperm_phase_3_param_5];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mad.lo.s32 	%r52, %r2, %r23, %r1;
	setp.ge.u32	%p3, %r52, %r21;
	@%p3 bra 	BB34_13;

	shl.b32 	%r24, %r1, 2;
	mov.u32 	%r25, shared;
	add.s32 	%r4, %r25, %r24;
	mov.u32 	%r26, WARP_SZ;
	add.s32 	%r27, %r26, -1;
	and.b32  	%r28, %r27, %r1;
	mov.u32 	%r29, 32;
	sub.s32 	%r5, %r29, %r28;
	mov.u32 	%r30, %nctaid.x;
	mul.lo.s32 	%r6, %r30, %r2;
	cvta.to.global.u64 	%rd10, %rd2;
	cvta.to.global.u64 	%rd11, %rd3;

BB34_2:
	setp.ge.u32	%p4, %r52, %r20;
	mov.u32 	%r53, %r22;
	@%p4 bra 	BB34_4;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.u32 	%rd5, %r52, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u32 	%r53, [%rd6];

BB34_4:
	add.s32 	%r32, %r1, 1;
	setp.lt.u32	%p1, %r32, %r2;
	st.shared.u32 	[%r4], %r53;
	bar.sync 	0;
	@%p1 bra 	BB34_7;
	bra.uni 	BB34_5;

BB34_7:
	ld.shared.u32 	%r54, [%r4+4];
	bra.uni 	BB34_8;

BB34_5:
	add.s32 	%r10, %r52, 1;
	setp.ge.u32	%p5, %r10, %r20;
	mov.u32 	%r54, %r22;
	@%p5 bra 	BB34_8;

	cvta.to.global.u64 	%rd7, %rd1;
	mul.wide.u32 	%rd8, %r10, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u32 	%r54, [%rd9];

BB34_8:
	setp.ne.s32	%p6, %r54, %r53;
	mov.u32 	%r34, -1;
	vote.sync.ballot.b32 	%r14, %p6, %r34;
	setp.eq.s32	%p7, %r54, %r53;
	@%p7 bra 	BB34_12;

	neg.s32 	%r36, %r14;
	mov.u32 	%r35, 0;
	and.b32  	%r37, %r14, %r36;
	clz.b32 	%r38, %r37;
	mov.u32 	%r39, 31;
	sub.s32 	%r16, %r39, %r38;
	setp.ne.s32	%p8, %r28, %r16;
	mov.u32 	%r55, %r35;
	@%p8 bra 	BB34_11;

	popc.b32 	%r44, %r14;
	atom.global.add.u32 	%r55, [%rd10], %r44;

BB34_11:
	shfl.sync.idx.b32 	%r46|%p9, %r55, %r16, %r39, %r14;
	shl.b32 	%r47, %r14, %r5;
	popc.b32 	%r48, %r47;
	add.s32 	%r49, %r46, %r48;
	mul.wide.u32 	%rd12, %r49, 16;
	add.s64 	%rd13, %rd11, %rd12;
	sub.s32 	%r50, %r54, %r53;
	st.global.v4.u32 	[%rd13], {%r52, %r53, %r50, %r35};

BB34_12:
	add.s32 	%r52, %r6, %r52;
	setp.lt.u32	%p10, %r52, %r21;
	@%p10 bra 	BB34_2;

BB34_13:
	ret;
}

	// .globl	mkperm_phase_4_tiny
.visible .entry mkperm_phase_4_tiny(
	.param .u64 mkperm_phase_4_tiny_param_0,
	.param .u64 mkperm_phase_4_tiny_param_1,
	.param .u64 mkperm_phase_4_tiny_param_2,
	.param .u32 mkperm_phase_4_tiny_param_3,
	.param .u32 mkperm_phase_4_tiny_param_4,
	.param .u32 mkperm_phase_4_tiny_param_5
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<55>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd4, [mkperm_phase_4_tiny_param_0];
	ld.param.u64 	%rd6, [mkperm_phase_4_tiny_param_1];
	ld.param.u64 	%rd5, [mkperm_phase_4_tiny_param_2];
	ld.param.u32 	%r22, [mkperm_phase_4_tiny_param_3];
	ld.param.u32 	%r24, [mkperm_phase_4_tiny_param_4];
	ld.param.u32 	%r23, [mkperm_phase_4_tiny_param_5];
	cvta.to.global.u64 	%rd1, %rd6;
	mov.u32 	%r25, %ctaid.x;
	mul.lo.s32 	%r1, %r25, %r24;
	add.s32 	%r2, %r1, %r24;
	mov.u32 	%r3, WARP_SZ;
	mov.u32 	%r4, %ntid.x;
	div.u32 	%r26, %r4, %r3;
	mov.u32 	%r5, %tid.x;
	div.u32 	%r6, %r5, %r3;
	mul.lo.s32 	%r27, %r25, %r23;
	mul.lo.s32 	%r28, %r27, %r26;
	cvt.u64.u32	%rd2, %r28;
	mul.lo.s32 	%r7, %r26, %r23;
	setp.ge.u32	%p2, %r5, %r7;
	@%p2 bra 	BB35_3;

	mov.u32 	%r52, %r5;

BB35_2:
	cvt.u64.u32	%rd7, %r52;
	add.s64 	%rd8, %rd7, %rd2;
	shl.b64 	%rd9, %rd8, 2;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.u32 	%r29, [%rd10];
	shl.b32 	%r30, %r52, 2;
	mov.u32 	%r31, shared;
	add.s32 	%r32, %r31, %r30;
	st.shared.u32 	[%r32], %r29;
	add.s32 	%r52, %r52, %r4;
	setp.lt.u32	%p3, %r52, %r7;
	@%p3 bra 	BB35_2;

BB35_3:
	bar.sync 	0;
	add.s32 	%r53, %r1, %r5;
	setp.ge.u32	%p4, %r53, %r2;
	@%p4 bra 	BB35_10;

	add.s32 	%r33, %r3, -1;
	and.b32  	%r11, %r33, %r5;
	mov.u32 	%r34, 32;
	sub.s32 	%r12, %r34, %r11;
	cvta.to.global.u64 	%rd3, %rd5;
	mul.lo.s32 	%r13, %r6, %r23;
	cvta.to.global.u64 	%rd11, %rd4;

BB35_5:
	setp.lt.u32	%p5, %r53, %r22;
	mov.u32 	%r35, -1;
	vote.sync.ballot.b32 	%r15, %p5, %r35;
	setp.ge.u32	%p6, %r53, %r22;
	@%p6 bra 	BB35_9;

	mul.wide.u32 	%rd12, %r53, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.u32 	%r16, [%rd13];
	match.any.sync.b32 	%r17, %r16, %r15;
	neg.s32 	%r37, %r17;
	mov.u32 	%r54, 0;
	and.b32  	%r38, %r17, %r37;
	clz.b32 	%r39, %r38;
	mov.u32 	%r40, 31;
	sub.s32 	%r18, %r40, %r39;
	setp.ne.s32	%p7, %r11, %r18;
	@%p7 bra 	BB35_8;

	add.s32 	%r41, %r16, %r13;
	shl.b32 	%r42, %r41, 2;
	mov.u32 	%r43, shared;
	add.s32 	%r44, %r43, %r42;
	popc.b32 	%r45, %r17;
	ld.shared.u32 	%r54, [%r44];
	add.s32 	%r46, %r45, %r54;
	st.shared.u32 	[%r44], %r46;

BB35_8:
	shfl.sync.idx.b32 	%r48|%p8, %r54, %r18, %r40, %r17;
	shl.b32 	%r49, %r17, %r12;
	popc.b32 	%r50, %r49;
	add.s32 	%r51, %r48, %r50;
	mul.wide.u32 	%rd14, %r51, 4;
	add.s64 	%rd15, %rd3, %rd14;
	st.global.u32 	[%rd15], %r53;

BB35_9:
	add.s32 	%r53, %r53, %r4;
	setp.lt.u32	%p9, %r53, %r2;
	@%p9 bra 	BB35_5;

BB35_10:
	ret;
}

	// .globl	mkperm_phase_4_small
.visible .entry mkperm_phase_4_small(
	.param .u64 mkperm_phase_4_small_param_0,
	.param .u64 mkperm_phase_4_small_param_1,
	.param .u64 mkperm_phase_4_small_param_2,
	.param .u32 mkperm_phase_4_small_param_3,
	.param .u32 mkperm_phase_4_small_param_4,
	.param .u32 mkperm_phase_4_small_param_5
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<48>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd5, [mkperm_phase_4_small_param_0];
	ld.param.u64 	%rd7, [mkperm_phase_4_small_param_1];
	ld.param.u64 	%rd6, [mkperm_phase_4_small_param_2];
	ld.param.u32 	%r18, [mkperm_phase_4_small_param_3];
	ld.param.u32 	%r20, [mkperm_phase_4_small_param_4];
	ld.param.u32 	%r19, [mkperm_phase_4_small_param_5];
	cvta.to.global.u64 	%rd1, %rd7;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r2, %r21, %r20;
	add.s32 	%r3, %r2, %r20;
	mul.lo.s32 	%r22, %r21, %r19;
	cvt.u64.u32	%rd2, %r22;
	mov.u32 	%r4, %tid.x;
	setp.ge.u32	%p2, %r4, %r19;
	@%p2 bra 	BB36_3;

	mov.u32 	%r45, %r4;

BB36_2:
	cvt.u64.u32	%rd8, %r45;
	add.s64 	%rd9, %rd8, %rd2;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.u32 	%r23, [%rd11];
	shl.b32 	%r24, %r45, 2;
	mov.u32 	%r25, shared;
	add.s32 	%r26, %r25, %r24;
	st.shared.u32 	[%r26], %r23;
	add.s32 	%r45, %r45, %r1;
	setp.lt.u32	%p3, %r45, %r19;
	@%p3 bra 	BB36_2;

BB36_3:
	bar.sync 	0;
	add.s32 	%r46, %r2, %r4;
	setp.ge.u32	%p4, %r46, %r3;
	@%p4 bra 	BB36_10;

	mov.u32 	%r27, WARP_SZ;
	add.s32 	%r28, %r27, -1;
	and.b32  	%r8, %r28, %r4;
	mov.u32 	%r29, 32;
	sub.s32 	%r9, %r29, %r8;
	cvta.to.global.u64 	%rd3, %rd6;
	cvta.to.global.u64 	%rd4, %rd5;

BB36_5:
	setp.lt.u32	%p5, %r46, %r18;
	mov.u32 	%r30, -1;
	vote.sync.ballot.b32 	%r11, %p5, %r30;
	setp.ge.u32	%p6, %r46, %r18;
	@%p6 bra 	BB36_9;

	mul.wide.u32 	%rd12, %r46, 4;
	add.s64 	%rd13, %rd4, %rd12;
	ld.global.u32 	%r12, [%rd13];
	match.any.sync.b32 	%r13, %r12, %r11;
	neg.s32 	%r32, %r13;
	mov.u32 	%r47, 0;
	and.b32  	%r33, %r13, %r32;
	clz.b32 	%r34, %r33;
	mov.u32 	%r35, 31;
	sub.s32 	%r14, %r35, %r34;
	setp.ne.s32	%p7, %r8, %r14;
	@%p7 bra 	BB36_8;

	popc.b32 	%r36, %r13;
	shl.b32 	%r37, %r12, 2;
	mov.u32 	%r38, shared;
	add.s32 	%r39, %r38, %r37;
	atom.shared.add.u32 	%r47, [%r39], %r36;

BB36_8:
	shfl.sync.idx.b32 	%r41|%p8, %r47, %r14, %r35, %r13;
	shl.b32 	%r42, %r13, %r9;
	popc.b32 	%r43, %r42;
	add.s32 	%r44, %r41, %r43;
	mul.wide.u32 	%rd14, %r44, 4;
	add.s64 	%rd15, %rd3, %rd14;
	st.global.u32 	[%rd15], %r46;

BB36_9:
	add.s32 	%r46, %r46, %r1;
	setp.lt.u32	%p9, %r46, %r3;
	@%p9 bra 	BB36_5;

BB36_10:
	ret;
}

	// .globl	mkperm_phase_4_large
.visible .entry mkperm_phase_4_large(
	.param .u64 mkperm_phase_4_large_param_0,
	.param .u64 mkperm_phase_4_large_param_1,
	.param .u64 mkperm_phase_4_large_param_2,
	.param .u32 mkperm_phase_4_large_param_3,
	.param .u32 mkperm_phase_4_large_param_4,
	.param .u32 mkperm_phase_4_large_param_5
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd2, [mkperm_phase_4_large_param_0];
	ld.param.u64 	%rd3, [mkperm_phase_4_large_param_1];
	ld.param.u64 	%rd4, [mkperm_phase_4_large_param_2];
	ld.param.u32 	%r13, [mkperm_phase_4_large_param_3];
	ld.param.u32 	%r15, [mkperm_phase_4_large_param_4];
	ld.param.u32 	%r14, [mkperm_phase_4_large_param_5];
	mov.u32 	%r16, %tid.x;
	mov.u32 	%r1, %ctaid.x;
	mul.lo.s32 	%r17, %r1, %r15;
	add.s32 	%r2, %r17, %r15;
	add.s32 	%r41, %r17, %r16;
	setp.ge.u32	%p2, %r41, %r2;
	@%p2 bra 	BB37_7;

	mov.u32 	%r18, WARP_SZ;
	add.s32 	%r19, %r18, -1;
	and.b32  	%r21, %r19, %r16;
	mov.u32 	%r22, 32;
	sub.s32 	%r4, %r22, %r21;
	mul.lo.s32 	%r23, %r1, %r14;
	cvt.u64.u32	%rd1, %r23;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd8, %rd3;
	cvta.to.global.u64 	%rd13, %rd4;

BB37_2:
	setp.lt.u32	%p3, %r41, %r13;
	mov.u32 	%r24, -1;
	vote.sync.ballot.b32 	%r6, %p3, %r24;
	setp.ge.u32	%p4, %r41, %r13;
	@%p4 bra 	BB37_6;

	mul.wide.u32 	%rd6, %r41, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u32 	%r7, [%rd7];
	match.any.sync.b32 	%r8, %r7, %r6;
	neg.s32 	%r26, %r8;
	mov.u32 	%r42, 0;
	and.b32  	%r27, %r8, %r26;
	clz.b32 	%r28, %r27;
	mov.u32 	%r29, 31;
	sub.s32 	%r9, %r29, %r28;
	setp.ne.s32	%p5, %r21, %r9;
	@%p5 bra 	BB37_5;

	popc.b32 	%r34, %r8;
	cvt.u64.u32	%rd9, %r7;
	add.s64 	%rd10, %rd9, %rd1;
	shl.b64 	%rd11, %rd10, 2;
	add.s64 	%rd12, %rd8, %rd11;
	atom.global.add.u32 	%r42, [%rd12], %r34;

BB37_5:
	shfl.sync.idx.b32 	%r36|%p6, %r42, %r9, %r29, %r8;
	shl.b32 	%r37, %r8, %r4;
	popc.b32 	%r38, %r37;
	add.s32 	%r39, %r36, %r38;
	mul.wide.u32 	%rd14, %r39, 4;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.u32 	[%rd15], %r41;

BB37_6:
	mov.u32 	%r40, %ntid.x;
	add.s32 	%r41, %r41, %r40;
	setp.lt.u32	%p7, %r41, %r2;
	@%p7 bra 	BB37_2;

BB37_7:
	ret;
}

	// .globl	transpose
.visible .entry transpose(
	.param .u64 transpose_param_0,
	.param .u64 transpose_param_1,
	.param .u32 transpose_param_2,
	.param .u32 transpose_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [transpose_param_0];
	ld.param.u64 	%rd2, [transpose_param_1];
	ld.param.u32 	%r9, [transpose_param_2];
	ld.param.u32 	%r10, [transpose_param_3];
	mov.u32 	%r11, %ctaid.x;
	shl.b32 	%r1, %r11, 4;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r3, %r1, %r2;
	mov.u32 	%r12, %ctaid.y;
	shl.b32 	%r4, %r12, 4;
	mov.u32 	%r5, %tid.y;
	add.s32 	%r6, %r4, %r5;
	setp.ge.u32	%p1, %r6, %r9;
	setp.ge.u32	%p2, %r3, %r10;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB38_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r13, %r6, %r10, %r3;
	mul.wide.u32 	%rd4, %r13, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r14, [%rd5];
	mad.lo.s32 	%r15, %r5, 17, %r2;
	shl.b32 	%r16, %r15, 2;
	mov.u32 	%r17, shared;
	add.s32 	%r18, %r17, %r16;
	st.shared.u32 	[%r18], %r14;

BB38_2:
	bar.sync 	0;
	add.s32 	%r7, %r5, %r1;
	setp.ge.u32	%p4, %r7, %r10;
	add.s32 	%r8, %r4, %r2;
	setp.ge.u32	%p5, %r8, %r9;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	BB38_4;

	mad.lo.s32 	%r19, %r2, 17, %r5;
	shl.b32 	%r20, %r19, 2;
	mov.u32 	%r21, shared;
	add.s32 	%r22, %r21, %r20;
	ld.shared.u32 	%r23, [%r22];
	mad.lo.s32 	%r24, %r7, %r9, %r8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r24, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r23;

BB38_4:
	ret;
}

	// .globl	poke_u8
.visible .entry poke_u8(
	.param .u64 poke_u8_param_0,
	.param .u8 poke_u8_param_1
)
{
	.reg .b16 	%rs<2>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [poke_u8_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	ld.param.u8 	%rs1, [poke_u8_param_1];
	st.global.u8 	[%rd2], %rs1;
	ret;
}

	// .globl	poke_u16
.visible .entry poke_u16(
	.param .u64 poke_u16_param_0,
	.param .u16 poke_u16_param_1
)
{
	.reg .b16 	%rs<2>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [poke_u16_param_0];
	ld.param.u16 	%rs1, [poke_u16_param_1];
	cvta.to.global.u64 	%rd2, %rd1;
	st.global.u16 	[%rd2], %rs1;
	ret;
}

	// .globl	poke_u32
.visible .entry poke_u32(
	.param .u64 poke_u32_param_0,
	.param .u32 poke_u32_param_1
)
{
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [poke_u32_param_0];
	ld.param.u32 	%r1, [poke_u32_param_1];
	cvta.to.global.u64 	%rd2, %rd1;
	st.global.u32 	[%rd2], %r1;
	ret;
}

	// .globl	poke_u64
.visible .entry poke_u64(
	.param .u64 poke_u64_param_0,
	.param .u64 poke_u64_param_1
)
{
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [poke_u64_param_0];
	ld.param.u64 	%rd2, [poke_u64_param_1];
	cvta.to.global.u64 	%rd3, %rd1;
	st.global.u64 	[%rd3], %rd2;
	ret;
}

	// .globl	fill_64
.visible .entry fill_64(
	.param .u64 fill_64_param_0,
	.param .u32 fill_64_param_1,
	.param .u64 fill_64_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd2, [fill_64_param_0];
	ld.param.u32 	%r6, [fill_64_param_1];
	ld.param.u64 	%rd3, [fill_64_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.u32	%p1, %r10, %r6;
	@%p1 bra 	BB43_3;

	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;
	cvta.to.global.u64 	%rd1, %rd2;

BB43_2:
	mul.wide.u32 	%rd4, %r10, 8;
	add.s64 	%rd5, %rd1, %rd4;
	st.global.u64 	[%rd5], %rd3;
	add.s32 	%r10, %r3, %r10;
	setp.lt.u32	%p2, %r10, %r6;
	@%p2 bra 	BB43_2;

BB43_3:
	ret;
}

	// .globl	vcall_prepare
.visible .entry vcall_prepare(
	.param .u64 vcall_prepare_param_0,
	.param .u64 vcall_prepare_param_1,
	.param .u32 vcall_prepare_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd5, [vcall_prepare_param_0];
	ld.param.u64 	%rd6, [vcall_prepare_param_1];
	ld.param.u32 	%r5, [vcall_prepare_param_2];
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r7, %r6, %r8;
	setp.ge.u32	%p1, %r1, %r5;
	@%p1 bra 	BB44_19;

	cvta.to.global.u64 	%rd7, %rd6;
	mul.wide.u32 	%rd8, %r1, 16;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.v2.u8 	{%rs8, %rs9}, [%rd9+4];
	ld.global.u64 	%rd1, [%rd9+8];
	ld.global.u32 	%rd10, [%rd9];
	cvta.to.global.u64 	%rd11, %rd5;
	add.s64 	%rd2, %rd11, %rd10;
	cvt.u32.u16	%r9, %rs8;
	setp.gt.s32	%p2, %r9, 3;
	@%p2 bra 	BB44_6;

	setp.eq.s32	%p5, %r9, 1;
	@%p5 bra 	BB44_15;
	bra.uni 	BB44_3;

BB44_15:
	setp.eq.s16	%p10, %rs9, 0;
	@%p10 bra 	BB44_17;

	cvt.u16.u64	%rs16, %rd1;
	bra.uni 	BB44_18;

BB44_6:
	setp.eq.s32	%p3, %r9, 4;
	@%p3 bra 	BB44_11;
	bra.uni 	BB44_7;

BB44_11:
	setp.eq.s16	%p8, %rs9, 0;
	@%p8 bra 	BB44_13;

	cvt.u32.u64	%r2, %rd1;
	st.global.u32 	[%rd2], %r2;
	bra.uni 	BB44_19;

BB44_3:
	setp.eq.s32	%p6, %r9, 2;
	@%p6 bra 	BB44_4;
	bra.uni 	BB44_19;

BB44_4:
	setp.eq.s16	%p9, %rs9, 0;
	@%p9 bra 	BB44_14;

	cvt.u16.u64	%rs2, %rd1;
	st.global.u16 	[%rd2], %rs2;
	bra.uni 	BB44_19;

BB44_7:
	setp.ne.s32	%p4, %r9, 8;
	@%p4 bra 	BB44_19;

	setp.ne.s16	%p7, %rs9, 0;
	@%p7 bra 	BB44_10;

	ld.u64 	%rd1, [%rd1];

BB44_10:
	st.global.u64 	[%rd2], %rd1;
	bra.uni 	BB44_19;

BB44_17:
	ld.u8 	%rs16, [%rd1];

BB44_18:
	st.global.u8 	[%rd2], %rs16;
	bra.uni 	BB44_19;

BB44_13:
	ld.u32 	%r3, [%rd1];
	st.global.u32 	[%rd2], %r3;
	bra.uni 	BB44_19;

BB44_14:
	ld.u16 	%rs3, [%rd1];
	st.global.u16 	[%rd2], %rs3;

BB44_19:
	ret;
}

	// .globl	block_copy_u32
.visible .entry block_copy_u32(
	.param .u64 block_copy_u32_param_0,
	.param .u64 block_copy_u32_param_1,
	.param .u32 block_copy_u32_param_2,
	.param .u32 block_copy_u32_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_copy_u32_param_0];
	ld.param.u64 	%rd2, [block_copy_u32_param_1];
	ld.param.u32 	%r3, [block_copy_u32_param_2];
	ld.param.u32 	%r2, [block_copy_u32_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.u32	%p1, %r1, %r3;
	@%p1 bra 	BB45_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r7, %r1, %r2;
	mul.wide.u32 	%rd4, %r7, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r8, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r8;

BB45_2:
	ret;
}

	// .globl	block_sum_u32
.visible .entry block_sum_u32(
	.param .u64 block_sum_u32_param_0,
	.param .u64 block_sum_u32_param_1,
	.param .u32 block_sum_u32_param_2,
	.param .u32 block_sum_u32_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_sum_u32_param_0];
	ld.param.u64 	%rd2, [block_sum_u32_param_1];
	ld.param.u32 	%r3, [block_sum_u32_param_2];
	ld.param.u32 	%r2, [block_sum_u32_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.u32	%p1, %r1, %r3;
	@%p1 bra 	BB46_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r7, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.u32 	%rd5, %r7, 4;
	add.s64 	%rd6, %rd4, %rd5;
	mul.wide.u32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r8, [%rd8];
	atom.global.add.u32 	%r9, [%rd6], %r8;

BB46_2:
	ret;
}

	// .globl	block_copy_u64
.visible .entry block_copy_u64(
	.param .u64 block_copy_u64_param_0,
	.param .u64 block_copy_u64_param_1,
	.param .u32 block_copy_u64_param_2,
	.param .u32 block_copy_u64_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd1, [block_copy_u64_param_0];
	ld.param.u64 	%rd2, [block_copy_u64_param_1];
	ld.param.u32 	%r3, [block_copy_u64_param_2];
	ld.param.u32 	%r2, [block_copy_u64_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.u32	%p1, %r1, %r3;
	@%p1 bra 	BB47_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r7, %r1, %r2;
	mul.wide.u32 	%rd4, %r7, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u64 	%rd6, [%rd5];
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.u32 	%rd8, %r1, 8;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u64 	[%rd9], %rd6;

BB47_2:
	ret;
}

	// .globl	block_sum_u64
.visible .entry block_sum_u64(
	.param .u64 block_sum_u64_param_0,
	.param .u64 block_sum_u64_param_1,
	.param .u32 block_sum_u64_param_2,
	.param .u32 block_sum_u64_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [block_sum_u64_param_0];
	ld.param.u64 	%rd2, [block_sum_u64_param_1];
	ld.param.u32 	%r3, [block_sum_u64_param_2];
	ld.param.u32 	%r2, [block_sum_u64_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.u32	%p1, %r1, %r3;
	@%p1 bra 	BB48_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r7, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.u32 	%rd5, %r7, 8;
	add.s64 	%rd6, %rd4, %rd5;
	mul.wide.u32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u64 	%rd9, [%rd8];
	atom.global.add.u64 	%rd10, [%rd6], %rd9;

BB48_2:
	ret;
}

	// .globl	block_copy_f32
.visible .entry block_copy_f32(
	.param .u64 block_copy_f32_param_0,
	.param .u64 block_copy_f32_param_1,
	.param .u32 block_copy_f32_param_2,
	.param .u32 block_copy_f32_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_copy_f32_param_0];
	ld.param.u64 	%rd2, [block_copy_f32_param_1];
	ld.param.u32 	%r3, [block_copy_f32_param_2];
	ld.param.u32 	%r2, [block_copy_f32_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.u32	%p1, %r1, %r3;
	@%p1 bra 	BB49_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r7, %r1, %r2;
	mul.wide.u32 	%rd4, %r7, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB49_2:
	ret;
}

	// .globl	block_sum_f32
.visible .entry block_sum_f32(
	.param .u64 block_sum_f32_param_0,
	.param .u64 block_sum_f32_param_1,
	.param .u32 block_sum_f32_param_2,
	.param .u32 block_sum_f32_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_sum_f32_param_0];
	ld.param.u64 	%rd2, [block_sum_f32_param_1];
	ld.param.u32 	%r3, [block_sum_f32_param_2];
	ld.param.u32 	%r2, [block_sum_f32_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.u32	%p1, %r1, %r3;
	@%p1 bra 	BB50_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r7, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.u32 	%rd5, %r7, 4;
	add.s64 	%rd6, %rd4, %rd5;
	mul.wide.u32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.f32 	%f1, [%rd8];
	atom.global.add.f32 	%f2, [%rd6], %f1;

BB50_2:
	ret;
}

	// .globl	block_copy_f64
.visible .entry block_copy_f64(
	.param .u64 block_copy_f64_param_0,
	.param .u64 block_copy_f64_param_1,
	.param .u32 block_copy_f64_param_2,
	.param .u32 block_copy_f64_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_copy_f64_param_0];
	ld.param.u64 	%rd2, [block_copy_f64_param_1];
	ld.param.u32 	%r3, [block_copy_f64_param_2];
	ld.param.u32 	%r2, [block_copy_f64_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.u32	%p1, %r1, %r3;
	@%p1 bra 	BB51_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r7, %r1, %r2;
	mul.wide.u32 	%rd4, %r7, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB51_2:
	ret;
}

	// .globl	block_sum_f64
.visible .entry block_sum_f64(
	.param .u64 block_sum_f64_param_0,
	.param .u64 block_sum_f64_param_1,
	.param .u32 block_sum_f64_param_2,
	.param .u32 block_sum_f64_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_sum_f64_param_0];
	ld.param.u64 	%rd2, [block_sum_f64_param_1];
	ld.param.u32 	%r3, [block_sum_f64_param_2];
	ld.param.u32 	%r2, [block_sum_f64_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r5, %r4, %r6;
	setp.ge.u32	%p1, %r1, %r3;
	@%p1 bra 	BB52_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r7, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.u32 	%rd5, %r7, 8;
	add.s64 	%rd6, %rd4, %rd5;
	mul.wide.u32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.f64 	%fd1, [%rd8];
	atom.global.add.f64 	%fd2, [%rd6], %fd1;

BB52_2:
	ret;
}


